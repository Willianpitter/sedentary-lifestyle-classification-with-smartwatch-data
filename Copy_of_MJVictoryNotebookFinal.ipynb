{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MJVictoryNotebookFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XOSghUGzIXyM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOSghUGzIXyM",
        "colab_type": "text"
      },
      "source": [
        "# Modelo de Análise Automatizada de Sedentarismo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSLCMUWpIjwP",
        "colab_type": "text"
      },
      "source": [
        "## Criando a Pasta de Armazanamento\n",
        "\n",
        "Usando o Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGj6b1Vwvuwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c084c705-e932-4166-8eb8-ed1487f45097"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jrFIytUp9Wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2818fad4-b62f-4b2b-d14f-0dec04418355"
      },
      "source": [
        "cd drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc-eacW8964k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d72b60f1-618e-41cc-9c5f-9d3d77c55716"
      },
      "source": [
        "cd wisdm-dataset/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/wisdm-dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_r7a8BB97uF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e2d3cc9-10b3-4c0f-d88a-b95ebdfbab9b"
      },
      "source": [
        "cd arff_files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/wisdm-dataset/arff_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvMRAPVw98He",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0e6c323-0676-4ad6-df40-7090539fdf36"
      },
      "source": [
        "cd watch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/wisdm-dataset/arff_files/watch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn5Dfky_9_VK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6185c0e-1dd3-476c-eb63-6150e2991040"
      },
      "source": [
        "cd accel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/wisdm-dataset/arff_files/watch/accel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eebJGCaXI6rV",
        "colab_type": "text"
      },
      "source": [
        "### Extensão para ler o DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0BKLM2h-EbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "a0235ed7-eaf0-47b5-a245-49452613c752"
      },
      "source": [
        "pip install arff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting arff\n",
            "  Downloading https://files.pythonhosted.org/packages/50/de/62d4446c5a6e459052c2f2d9490c370ddb6abc0766547b4cef585913598d/arff-0.9.tar.gz\n",
            "Building wheels for collected packages: arff\n",
            "  Building wheel for arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arff: filename=arff-0.9-cp36-none-any.whl size=4970 sha256=64cb6e703d2734524b5cc90283cfe8a2fc1f972fcbdd828bbeaccdc624b307cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/d0/70/2c73afedd3ac25c6085b528742c69b9587cbdfa67e5194583b\n",
            "Successfully built arff\n",
            "Installing collected packages: arff\n",
            "Successfully installed arff-0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "875j2OEDEhsV",
        "colab_type": "text"
      },
      "source": [
        "##Loading files to a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0uHUpkvss4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "139981b7-8cb7-4f43-9e17-132612c90832"
      },
      "source": [
        "import pandas as pd\n",
        "import arff\n",
        "df_concat = pd.DataFrame()\n",
        "for i in range(1600,1651):\n",
        "  file = 'data_{}_accel_watch.arff'.format(i)\n",
        "  #print(file)\n",
        "  try:\n",
        "    data = arff.load(file)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_concat = pd.concat([df_concat,df])\n",
        "  except OSError:\n",
        "    print(\"Este arquivo não existe\")\n",
        "df_concat.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Este arquivo não existe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0689</td>\n",
              "      <td>-1.13570</td>\n",
              "      <td>-0.022859</td>\n",
              "      <td>52.7027</td>\n",
              "      <td>46.3415</td>\n",
              "      <td>44.8837</td>\n",
              "      <td>3.33371</td>\n",
              "      <td>1.87644</td>\n",
              "      <td>2.00437</td>\n",
              "      <td>...</td>\n",
              "      <td>0.618168</td>\n",
              "      <td>0.613121</td>\n",
              "      <td>0.607417</td>\n",
              "      <td>0.601063</td>\n",
              "      <td>0.594066</td>\n",
              "      <td>0.586432</td>\n",
              "      <td>0.393334</td>\n",
              "      <td>0.555961</td>\n",
              "      <td>0.555067</td>\n",
              "      <td>0.553580</td>\n",
              "      <td>0.551500</td>\n",
              "      <td>0.548829</td>\n",
              "      <td>0.545570</td>\n",
              "      <td>0.541727</td>\n",
              "      <td>0.537304</td>\n",
              "      <td>0.532306</td>\n",
              "      <td>0.526738</td>\n",
              "      <td>0.520606</td>\n",
              "      <td>0.513916</td>\n",
              "      <td>0.381289</td>\n",
              "      <td>0.538935</td>\n",
              "      <td>0.538069</td>\n",
              "      <td>0.536627</td>\n",
              "      <td>0.534610</td>\n",
              "      <td>0.532021</td>\n",
              "      <td>0.528862</td>\n",
              "      <td>0.525137</td>\n",
              "      <td>0.520850</td>\n",
              "      <td>0.516005</td>\n",
              "      <td>0.510607</td>\n",
              "      <td>0.504662</td>\n",
              "      <td>0.498178</td>\n",
              "      <td>-0.395887</td>\n",
              "      <td>-0.105039</td>\n",
              "      <td>-0.263814</td>\n",
              "      <td>-0.035912</td>\n",
              "      <td>-0.305539</td>\n",
              "      <td>-0.292265</td>\n",
              "      <td>12.7783</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.360</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.1521</td>\n",
              "      <td>-1.75208</td>\n",
              "      <td>-1.327610</td>\n",
              "      <td>39.3750</td>\n",
              "      <td>40.0000</td>\n",
              "      <td>39.1837</td>\n",
              "      <td>2.50595</td>\n",
              "      <td>2.35428</td>\n",
              "      <td>1.67891</td>\n",
              "      <td>...</td>\n",
              "      <td>0.634539</td>\n",
              "      <td>0.629358</td>\n",
              "      <td>0.623504</td>\n",
              "      <td>0.616982</td>\n",
              "      <td>0.609799</td>\n",
              "      <td>0.601963</td>\n",
              "      <td>0.424565</td>\n",
              "      <td>0.600104</td>\n",
              "      <td>0.599140</td>\n",
              "      <td>0.597534</td>\n",
              "      <td>0.595289</td>\n",
              "      <td>0.592406</td>\n",
              "      <td>0.588888</td>\n",
              "      <td>0.584740</td>\n",
              "      <td>0.579966</td>\n",
              "      <td>0.574571</td>\n",
              "      <td>0.568561</td>\n",
              "      <td>0.561942</td>\n",
              "      <td>0.554721</td>\n",
              "      <td>0.375576</td>\n",
              "      <td>0.530860</td>\n",
              "      <td>0.530007</td>\n",
              "      <td>0.528587</td>\n",
              "      <td>0.526600</td>\n",
              "      <td>0.524050</td>\n",
              "      <td>0.520939</td>\n",
              "      <td>0.517269</td>\n",
              "      <td>0.513046</td>\n",
              "      <td>0.508274</td>\n",
              "      <td>0.502957</td>\n",
              "      <td>0.497101</td>\n",
              "      <td>0.490714</td>\n",
              "      <td>-0.517585</td>\n",
              "      <td>-0.493655</td>\n",
              "      <td>0.208926</td>\n",
              "      <td>-0.166636</td>\n",
              "      <td>0.071774</td>\n",
              "      <td>-0.070860</td>\n",
              "      <td>12.8712</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.1529</td>\n",
              "      <td>-1.53875</td>\n",
              "      <td>-0.972243</td>\n",
              "      <td>43.4091</td>\n",
              "      <td>37.8846</td>\n",
              "      <td>39.7917</td>\n",
              "      <td>2.45466</td>\n",
              "      <td>1.76268</td>\n",
              "      <td>1.37839</td>\n",
              "      <td>...</td>\n",
              "      <td>0.616762</td>\n",
              "      <td>0.611726</td>\n",
              "      <td>0.606036</td>\n",
              "      <td>0.599696</td>\n",
              "      <td>0.592714</td>\n",
              "      <td>0.585098</td>\n",
              "      <td>0.371445</td>\n",
              "      <td>0.525021</td>\n",
              "      <td>0.524177</td>\n",
              "      <td>0.522773</td>\n",
              "      <td>0.520808</td>\n",
              "      <td>0.518286</td>\n",
              "      <td>0.515208</td>\n",
              "      <td>0.511580</td>\n",
              "      <td>0.507403</td>\n",
              "      <td>0.502683</td>\n",
              "      <td>0.497424</td>\n",
              "      <td>0.491633</td>\n",
              "      <td>0.485316</td>\n",
              "      <td>0.330943</td>\n",
              "      <td>0.467774</td>\n",
              "      <td>0.467023</td>\n",
              "      <td>0.465771</td>\n",
              "      <td>0.464021</td>\n",
              "      <td>0.461773</td>\n",
              "      <td>0.459032</td>\n",
              "      <td>0.455798</td>\n",
              "      <td>0.452077</td>\n",
              "      <td>0.447872</td>\n",
              "      <td>0.443187</td>\n",
              "      <td>0.438027</td>\n",
              "      <td>0.432398</td>\n",
              "      <td>-0.573148</td>\n",
              "      <td>-0.468498</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>-0.090878</td>\n",
              "      <td>0.005053</td>\n",
              "      <td>-0.382557</td>\n",
              "      <td>12.5949</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.9523</td>\n",
              "      <td>-1.31171</td>\n",
              "      <td>-0.505159</td>\n",
              "      <td>45.9524</td>\n",
              "      <td>45.0000</td>\n",
              "      <td>38.9796</td>\n",
              "      <td>2.67660</td>\n",
              "      <td>1.68549</td>\n",
              "      <td>1.40901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.613069</td>\n",
              "      <td>0.608064</td>\n",
              "      <td>0.602407</td>\n",
              "      <td>0.596106</td>\n",
              "      <td>0.589166</td>\n",
              "      <td>0.581595</td>\n",
              "      <td>0.376999</td>\n",
              "      <td>0.532871</td>\n",
              "      <td>0.532015</td>\n",
              "      <td>0.530589</td>\n",
              "      <td>0.528596</td>\n",
              "      <td>0.526036</td>\n",
              "      <td>0.522912</td>\n",
              "      <td>0.519229</td>\n",
              "      <td>0.514990</td>\n",
              "      <td>0.510199</td>\n",
              "      <td>0.504862</td>\n",
              "      <td>0.498985</td>\n",
              "      <td>0.492573</td>\n",
              "      <td>0.338095</td>\n",
              "      <td>0.477883</td>\n",
              "      <td>0.477115</td>\n",
              "      <td>0.475837</td>\n",
              "      <td>0.474048</td>\n",
              "      <td>0.471753</td>\n",
              "      <td>0.468952</td>\n",
              "      <td>0.465649</td>\n",
              "      <td>0.461847</td>\n",
              "      <td>0.457550</td>\n",
              "      <td>0.452764</td>\n",
              "      <td>0.447493</td>\n",
              "      <td>0.441743</td>\n",
              "      <td>-0.514097</td>\n",
              "      <td>-0.232246</td>\n",
              "      <td>-0.080241</td>\n",
              "      <td>-0.117089</td>\n",
              "      <td>0.137656</td>\n",
              "      <td>-0.265747</td>\n",
              "      <td>12.3521</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.540</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0245</td>\n",
              "      <td>-1.03888</td>\n",
              "      <td>0.240671</td>\n",
              "      <td>40.6522</td>\n",
              "      <td>40.8511</td>\n",
              "      <td>45.0000</td>\n",
              "      <td>2.36682</td>\n",
              "      <td>1.54154</td>\n",
              "      <td>1.55086</td>\n",
              "      <td>...</td>\n",
              "      <td>0.595258</td>\n",
              "      <td>0.590398</td>\n",
              "      <td>0.584906</td>\n",
              "      <td>0.578787</td>\n",
              "      <td>0.572049</td>\n",
              "      <td>0.564698</td>\n",
              "      <td>0.357504</td>\n",
              "      <td>0.505316</td>\n",
              "      <td>0.504504</td>\n",
              "      <td>0.503152</td>\n",
              "      <td>0.501261</td>\n",
              "      <td>0.498833</td>\n",
              "      <td>0.495872</td>\n",
              "      <td>0.492379</td>\n",
              "      <td>0.488359</td>\n",
              "      <td>0.483816</td>\n",
              "      <td>0.478755</td>\n",
              "      <td>0.473181</td>\n",
              "      <td>0.467101</td>\n",
              "      <td>0.327375</td>\n",
              "      <td>0.462730</td>\n",
              "      <td>0.461987</td>\n",
              "      <td>0.460749</td>\n",
              "      <td>0.459017</td>\n",
              "      <td>0.456794</td>\n",
              "      <td>0.454082</td>\n",
              "      <td>0.450884</td>\n",
              "      <td>0.447202</td>\n",
              "      <td>0.443042</td>\n",
              "      <td>0.438408</td>\n",
              "      <td>0.433304</td>\n",
              "      <td>0.427736</td>\n",
              "      <td>-0.461254</td>\n",
              "      <td>0.084470</td>\n",
              "      <td>-0.418888</td>\n",
              "      <td>-0.097265</td>\n",
              "      <td>-0.118795</td>\n",
              "      <td>-0.414198</td>\n",
              "      <td>12.4168</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  0    1    2    3      4   ...        88        89        90       91    92\n",
              "0  A  0.0  0.0  0.0  0.045  ... -0.035912 -0.305539 -0.292265  12.7783  1600\n",
              "1  A  0.0  0.0  0.0  0.000  ... -0.166636  0.071774 -0.070860  12.8712  1600\n",
              "2  A  0.0  0.0  0.0  0.000  ... -0.090878  0.005053 -0.382557  12.5949  1600\n",
              "3  A  0.0  0.0  0.0  0.005  ... -0.117089  0.137656 -0.265747  12.3521  1600\n",
              "4  A  0.0  0.0  0.0  0.000  ... -0.097265 -0.118795 -0.414198  12.4168  1600\n",
              "\n",
              "[5 rows x 93 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhgJGpGtWwSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acoes_irrelevantes = ['F','G','H','I','J','K','L','Q','R','S']\n",
        "for acoes in acoes_irrelevantes:\n",
        "  df_concat = df_concat[df_concat[0] != acoes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig4xBJ7vKjqN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik7rpNUInlkW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "a88291ba-fb47-41f1-9a9a-cc120f51dd8a"
      },
      "source": [
        "df_concat.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0689</td>\n",
              "      <td>-1.13570</td>\n",
              "      <td>-0.022859</td>\n",
              "      <td>52.7027</td>\n",
              "      <td>46.3415</td>\n",
              "      <td>44.8837</td>\n",
              "      <td>3.33371</td>\n",
              "      <td>1.87644</td>\n",
              "      <td>2.00437</td>\n",
              "      <td>...</td>\n",
              "      <td>0.618168</td>\n",
              "      <td>0.613121</td>\n",
              "      <td>0.607417</td>\n",
              "      <td>0.601063</td>\n",
              "      <td>0.594066</td>\n",
              "      <td>0.586432</td>\n",
              "      <td>0.393334</td>\n",
              "      <td>0.555961</td>\n",
              "      <td>0.555067</td>\n",
              "      <td>0.553580</td>\n",
              "      <td>0.551500</td>\n",
              "      <td>0.548829</td>\n",
              "      <td>0.545570</td>\n",
              "      <td>0.541727</td>\n",
              "      <td>0.537304</td>\n",
              "      <td>0.532306</td>\n",
              "      <td>0.526738</td>\n",
              "      <td>0.520606</td>\n",
              "      <td>0.513916</td>\n",
              "      <td>0.381289</td>\n",
              "      <td>0.538935</td>\n",
              "      <td>0.538069</td>\n",
              "      <td>0.536627</td>\n",
              "      <td>0.534610</td>\n",
              "      <td>0.532021</td>\n",
              "      <td>0.528862</td>\n",
              "      <td>0.525137</td>\n",
              "      <td>0.520850</td>\n",
              "      <td>0.516005</td>\n",
              "      <td>0.510607</td>\n",
              "      <td>0.504662</td>\n",
              "      <td>0.498178</td>\n",
              "      <td>-0.395887</td>\n",
              "      <td>-0.105039</td>\n",
              "      <td>-0.263814</td>\n",
              "      <td>-0.035912</td>\n",
              "      <td>-0.305539</td>\n",
              "      <td>-0.292265</td>\n",
              "      <td>12.7783</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.360</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.1521</td>\n",
              "      <td>-1.75208</td>\n",
              "      <td>-1.327610</td>\n",
              "      <td>39.3750</td>\n",
              "      <td>40.0000</td>\n",
              "      <td>39.1837</td>\n",
              "      <td>2.50595</td>\n",
              "      <td>2.35428</td>\n",
              "      <td>1.67891</td>\n",
              "      <td>...</td>\n",
              "      <td>0.634539</td>\n",
              "      <td>0.629358</td>\n",
              "      <td>0.623504</td>\n",
              "      <td>0.616982</td>\n",
              "      <td>0.609799</td>\n",
              "      <td>0.601963</td>\n",
              "      <td>0.424565</td>\n",
              "      <td>0.600104</td>\n",
              "      <td>0.599140</td>\n",
              "      <td>0.597534</td>\n",
              "      <td>0.595289</td>\n",
              "      <td>0.592406</td>\n",
              "      <td>0.588888</td>\n",
              "      <td>0.584740</td>\n",
              "      <td>0.579966</td>\n",
              "      <td>0.574571</td>\n",
              "      <td>0.568561</td>\n",
              "      <td>0.561942</td>\n",
              "      <td>0.554721</td>\n",
              "      <td>0.375576</td>\n",
              "      <td>0.530860</td>\n",
              "      <td>0.530007</td>\n",
              "      <td>0.528587</td>\n",
              "      <td>0.526600</td>\n",
              "      <td>0.524050</td>\n",
              "      <td>0.520939</td>\n",
              "      <td>0.517269</td>\n",
              "      <td>0.513046</td>\n",
              "      <td>0.508274</td>\n",
              "      <td>0.502957</td>\n",
              "      <td>0.497101</td>\n",
              "      <td>0.490714</td>\n",
              "      <td>-0.517585</td>\n",
              "      <td>-0.493655</td>\n",
              "      <td>0.208926</td>\n",
              "      <td>-0.166636</td>\n",
              "      <td>0.071774</td>\n",
              "      <td>-0.070860</td>\n",
              "      <td>12.8712</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.1529</td>\n",
              "      <td>-1.53875</td>\n",
              "      <td>-0.972243</td>\n",
              "      <td>43.4091</td>\n",
              "      <td>37.8846</td>\n",
              "      <td>39.7917</td>\n",
              "      <td>2.45466</td>\n",
              "      <td>1.76268</td>\n",
              "      <td>1.37839</td>\n",
              "      <td>...</td>\n",
              "      <td>0.616762</td>\n",
              "      <td>0.611726</td>\n",
              "      <td>0.606036</td>\n",
              "      <td>0.599696</td>\n",
              "      <td>0.592714</td>\n",
              "      <td>0.585098</td>\n",
              "      <td>0.371445</td>\n",
              "      <td>0.525021</td>\n",
              "      <td>0.524177</td>\n",
              "      <td>0.522773</td>\n",
              "      <td>0.520808</td>\n",
              "      <td>0.518286</td>\n",
              "      <td>0.515208</td>\n",
              "      <td>0.511580</td>\n",
              "      <td>0.507403</td>\n",
              "      <td>0.502683</td>\n",
              "      <td>0.497424</td>\n",
              "      <td>0.491633</td>\n",
              "      <td>0.485316</td>\n",
              "      <td>0.330943</td>\n",
              "      <td>0.467774</td>\n",
              "      <td>0.467023</td>\n",
              "      <td>0.465771</td>\n",
              "      <td>0.464021</td>\n",
              "      <td>0.461773</td>\n",
              "      <td>0.459032</td>\n",
              "      <td>0.455798</td>\n",
              "      <td>0.452077</td>\n",
              "      <td>0.447872</td>\n",
              "      <td>0.443187</td>\n",
              "      <td>0.438027</td>\n",
              "      <td>0.432398</td>\n",
              "      <td>-0.573148</td>\n",
              "      <td>-0.468498</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>-0.090878</td>\n",
              "      <td>0.005053</td>\n",
              "      <td>-0.382557</td>\n",
              "      <td>12.5949</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.9523</td>\n",
              "      <td>-1.31171</td>\n",
              "      <td>-0.505159</td>\n",
              "      <td>45.9524</td>\n",
              "      <td>45.0000</td>\n",
              "      <td>38.9796</td>\n",
              "      <td>2.67660</td>\n",
              "      <td>1.68549</td>\n",
              "      <td>1.40901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.613069</td>\n",
              "      <td>0.608064</td>\n",
              "      <td>0.602407</td>\n",
              "      <td>0.596106</td>\n",
              "      <td>0.589166</td>\n",
              "      <td>0.581595</td>\n",
              "      <td>0.376999</td>\n",
              "      <td>0.532871</td>\n",
              "      <td>0.532015</td>\n",
              "      <td>0.530589</td>\n",
              "      <td>0.528596</td>\n",
              "      <td>0.526036</td>\n",
              "      <td>0.522912</td>\n",
              "      <td>0.519229</td>\n",
              "      <td>0.514990</td>\n",
              "      <td>0.510199</td>\n",
              "      <td>0.504862</td>\n",
              "      <td>0.498985</td>\n",
              "      <td>0.492573</td>\n",
              "      <td>0.338095</td>\n",
              "      <td>0.477883</td>\n",
              "      <td>0.477115</td>\n",
              "      <td>0.475837</td>\n",
              "      <td>0.474048</td>\n",
              "      <td>0.471753</td>\n",
              "      <td>0.468952</td>\n",
              "      <td>0.465649</td>\n",
              "      <td>0.461847</td>\n",
              "      <td>0.457550</td>\n",
              "      <td>0.452764</td>\n",
              "      <td>0.447493</td>\n",
              "      <td>0.441743</td>\n",
              "      <td>-0.514097</td>\n",
              "      <td>-0.232246</td>\n",
              "      <td>-0.080241</td>\n",
              "      <td>-0.117089</td>\n",
              "      <td>0.137656</td>\n",
              "      <td>-0.265747</td>\n",
              "      <td>12.3521</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.540</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0245</td>\n",
              "      <td>-1.03888</td>\n",
              "      <td>0.240671</td>\n",
              "      <td>40.6522</td>\n",
              "      <td>40.8511</td>\n",
              "      <td>45.0000</td>\n",
              "      <td>2.36682</td>\n",
              "      <td>1.54154</td>\n",
              "      <td>1.55086</td>\n",
              "      <td>...</td>\n",
              "      <td>0.595258</td>\n",
              "      <td>0.590398</td>\n",
              "      <td>0.584906</td>\n",
              "      <td>0.578787</td>\n",
              "      <td>0.572049</td>\n",
              "      <td>0.564698</td>\n",
              "      <td>0.357504</td>\n",
              "      <td>0.505316</td>\n",
              "      <td>0.504504</td>\n",
              "      <td>0.503152</td>\n",
              "      <td>0.501261</td>\n",
              "      <td>0.498833</td>\n",
              "      <td>0.495872</td>\n",
              "      <td>0.492379</td>\n",
              "      <td>0.488359</td>\n",
              "      <td>0.483816</td>\n",
              "      <td>0.478755</td>\n",
              "      <td>0.473181</td>\n",
              "      <td>0.467101</td>\n",
              "      <td>0.327375</td>\n",
              "      <td>0.462730</td>\n",
              "      <td>0.461987</td>\n",
              "      <td>0.460749</td>\n",
              "      <td>0.459017</td>\n",
              "      <td>0.456794</td>\n",
              "      <td>0.454082</td>\n",
              "      <td>0.450884</td>\n",
              "      <td>0.447202</td>\n",
              "      <td>0.443042</td>\n",
              "      <td>0.438408</td>\n",
              "      <td>0.433304</td>\n",
              "      <td>0.427736</td>\n",
              "      <td>-0.461254</td>\n",
              "      <td>0.084470</td>\n",
              "      <td>-0.418888</td>\n",
              "      <td>-0.097265</td>\n",
              "      <td>-0.118795</td>\n",
              "      <td>-0.414198</td>\n",
              "      <td>12.4168</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  0    1    2    3      4   ...        88        89        90       91    92\n",
              "0  A  0.0  0.0  0.0  0.045  ... -0.035912 -0.305539 -0.292265  12.7783  1600\n",
              "1  A  0.0  0.0  0.0  0.000  ... -0.166636  0.071774 -0.070860  12.8712  1600\n",
              "2  A  0.0  0.0  0.0  0.000  ... -0.090878  0.005053 -0.382557  12.5949  1600\n",
              "3  A  0.0  0.0  0.0  0.005  ... -0.117089  0.137656 -0.265747  12.3521  1600\n",
              "4  A  0.0  0.0  0.0  0.000  ... -0.097265 -0.118795 -0.414198  12.4168  1600\n",
              "\n",
              "[5 rows x 93 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYZEkamyVG1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_concat = df_concat.replace('A','CAMINHADA')\n",
        "df_concat = df_concat.replace('B','MODERADA')\n",
        "df_concat = df_concat.replace('C','MODERADA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVobEpnsY4O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_concat = df_concat.replace('M','VIGOROSA')\n",
        "df_concat = df_concat.replace('O','VIGOROSA')\n",
        "df_concat = df_concat.replace('P','VIGOROSA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4xoZ1wKZJxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_concat = df_concat.replace('D','INATIVO')\n",
        "df_concat = df_concat.replace('E','INATIVO')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMo6NNeIL0Ul",
        "colab_type": "text"
      },
      "source": [
        "##Frequencia das atividades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3a3kVcaL0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''cont_MODERADA=0\n",
        "cont_INATIVO=0\n",
        "cont_VIGORASA=0\n",
        "cont_CAMINHADA = 0\n",
        "\n",
        "for i in range(0,df_concat.shape[0]):\n",
        "  if(df_concat.iloc[i,0]=='MODERADA'):\n",
        "    cont_MODERADA+=1\n",
        "  if(df_concat.iloc[i,0]=='INATIVO'):\n",
        "    cont_INATIVO+=1\n",
        "  if(df_concat.iloc[i,0]=='VIGOROSA'):\n",
        "    cont_VIGORASA+=1\n",
        "  if(df_concat.iloc[i,0]=='CAMINHADA'):\n",
        "    cont_CAMINHADA+=1'''\n",
        "\n",
        "#SUGESTAO\n",
        "cont_MODERADA  = len(df_concat[df_concat[0] == 'MODERADA'][0])\n",
        "cont_INATIVO   = len(df_concat[df_concat[0] == 'INATIVO'][0])\n",
        "cont_VIGORASA  = len(df_concat[df_concat[0] == 'VIGOROSA'][0])\n",
        "cont_CAMINHADA = len(df_concat[df_concat[0] == 'CAMINHADA'][0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UkRwaE1MEAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "663d42b7-52a3-4207-8f0d-019b84af5287"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "activity = [\"CAMINHADA\",\"VIGOROSA\",\"MODERADA\",\"INATIVO\"]\n",
        "y_pos = [cont_CAMINHADA,cont_VIGORASA,cont_MODERADA,cont_INATIVO]\n",
        "\n",
        "plt.bar(activity,y_pos)\n",
        "plt.xticks(activity)\n",
        "plt.ylabel('Quantidade')\n",
        "plt.title(\"Frequencia das Atividades\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEICAYAAACuxNj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLePvV70MVI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inicialização dos vetores para as acurácias e erros\n",
        "vector_plot =[]\n",
        "vector_plot_error=[]\n",
        "vector_training=['Random Forest','Random Forest + SelectKBest_75','Random Forest + PCA','Neural Network','Neural Network + SelectKBest_75','Neural Network + PCA']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVI6zaMFEZr1",
        "colab_type": "text"
      },
      "source": [
        "##Agrupando ações relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzKbidSWEZWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "#É preciso fazer one hot encoding antes de fazer o one hot enconding do Keras (por que deus???)\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "le = LabelEncoder() \n",
        "df_concat[0]= le.fit_transform(df_concat[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpiz9ltmNPbl",
        "colab_type": "text"
      },
      "source": [
        "##Sem Seleção de Atributos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqmB0ZccNXxK",
        "colab_type": "text"
      },
      "source": [
        "###Matriz Confusão da Random Forest sem Seleção de Atributos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhCDW_5gNOyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_values = df_concat.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEO4IGDeNfv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_forest  = RandomForestClassifier()\n",
        "random_forest = random_forest.fit(X_train, y_train)\n",
        "y_predit = random_forest.predict(X_test)\n",
        "cm = confusion_matrix(y_test,y_predit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVDaIAsENkmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "944d1af3-cf2c-4d10-a55b-67f53e178223"
      },
      "source": [
        "index = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "columns = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "#Porque este cm_matrix ta aqui\n",
        "cm_matrix = confusion_matrix(y_test,y_predit)\n",
        "#print(cm_matrix)\n",
        "#cm = pd.DataFrame(cm_matrix,index,columns)\n",
        "print(cm)\n",
        "cm = [[j/sum(i) for j in i] for i in cm] #transformando em porcentagem \n",
        "cm_df = pd.DataFrame(cm,index,columns)                    \n",
        "plt.figure(figsize=(10,5))  \n",
        "sns.heatmap(cm_df, annot=True,fmt=\".2%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[220   1  25  16]\n",
            " [  0 479   2  10]\n",
            " [ 16   1 449  25]\n",
            " [  3   6  21 758]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fceb2afbc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAEvCAYAAAAKDcjfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdnhnp6ENoBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#precisão\n",
        "precisao_inativo = cm_matrix[0][0]/(cm_matrix[0][0]+cm_matrix[0][1]+cm_matrix[0][2])\n",
        "precisao_moderada = cm_matrix[1][1]/(cm_matrix[1][0]+cm_matrix[1][1]+cm_matrix[1][2])\n",
        "precisao_vigorosa = cm_matrix[2][2]/(cm_matrix[2][0]+cm_matrix[2][1]+cm_matrix[2][2])\n",
        "#Sensibilidade\n",
        "sensibilidade_inativo = cm_matrix[0][0]/(cm_matrix[0][0]+cm_matrix[1][0]+cm_matrix[2][0])\n",
        "sensibilidade_moderada = cm_matrix[1][1]/(cm_matrix[0][1]+cm_matrix[1][1]+cm_matrix[2][1])\n",
        "sensibilidade_vigorosa = cm_matrix[2][2]/(cm_matrix[0][2]+cm_matrix[1][2]+cm_matrix[2][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qX86froNsPd",
        "colab_type": "text"
      },
      "source": [
        "##Random Forest KFold Sem Seleção de Atributos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TArD1omGNwM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "df_values = df_concat.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "groups = df_concat[92] #ids\n",
        "scores = cross_val_score(RandomForestClassifier(n_estimators=100,max_depth=10,random_state=1,criterion='entropy'), df_values, df_target, cv=49,scoring=\"accuracy\",groups=groups)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "vector_plot.append(scores.mean())\n",
        "vector_plot_error.append(scores.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTA8zwvd-HWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(vet,vet_aux)\n",
        "plt.xticks(vet)\n",
        "plt.ylabel('Acuracia')\n",
        "plt.title(\"Frequencia das Acurácias\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtfNHsMXN7oH",
        "colab_type": "text"
      },
      "source": [
        "#Matriz Confusão da Rede Neural Sem Seleção de Atributos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3URuvcWN66z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create first network with Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from keras.utils import to_categorical\n",
        "import keras\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "#One hot encoding\n",
        "df_values = df_concat.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "#print(df_target)\n",
        "df_target = to_categorical(df_target)# split into input (X) and output (Y) variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, random_state=0)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(92, input_dim=92, init= 'uniform', activation='sigmoid'))\n",
        "model.add(Dense(4,activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='poisson',optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, nb_epoch=50, batch_size=64, validation_data = (X_test,y_test))\n",
        "# evaluate the model\n",
        "#scores = model.evaluate(X, Y)\n",
        "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcnW9oAaOlaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict_classes(X_test)\n",
        "from numpy import argmax\n",
        "y_test_labled = []\n",
        "for i in range (len(y_test)):\n",
        "  y_test_labled.append(argmax(y_test[i]))\n",
        "cm = confusion_matrix(y_test_labled,y_predit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPHLYmjXOoKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "columns = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "cm_matrix = confusion_matrix(y_test_labled,y_predit)\n",
        "cm = [[j/sum(i) for j in i] for i in cm] #transformando em porcentagem \n",
        "cm = pd.DataFrame(cm,index,columns)                      \n",
        "plt.figure(figsize=(10,5))  \n",
        "sns.heatmap(cm, annot=True,fmt=\".2%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia0xHFn9Ow9e",
        "colab_type": "text"
      },
      "source": [
        "#Rede Neural KFold Sem Seleção de Atributos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEifb54LO5f6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from itertools import product\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Create first network with Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from keras.utils.np_utils import to_categorical\n",
        "import keras\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "#Validação cruzada separando por indivíduo -- Sugestão da equipe MJV\n",
        "groups = df_concat[92] #ids\n",
        "df_values = df_concat.drop(columns=0)\n",
        "df_values_np =df_values.to_numpy()\n",
        "df_target = df_concat[0]\n",
        "\n",
        "gkf = GroupKFold(n_splits=49)\n",
        "cvscores = []\n",
        "for train, test in gkf.split(df_values, df_target, groups=groups):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(92, input_dim=92, init= 'uniform', activation='sigmoid'))\n",
        "  model.add(Dense(4,activation='softmax'))\n",
        "  df_target_categorial = to_categorical(df_target)# split into input (X) and output (Y) variables\n",
        "  print(len(df_target_categorial[0]))\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(loss='poisson',optimizer='adam', metrics=['accuracy'])\n",
        "  # Fit the model\n",
        "  model.fit(df_values_np[train], df_target_categorial[train], nb_epoch=50, batch_size=64)\n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(df_values_np[test], df_target_categorial[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]))\n",
        "  cvscores.append(scores[1])\n",
        "\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
        "vector_plot.append(numpy.mean(cvscores))\n",
        "vector_plot_error.append(numpy.std(cvscores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj5JNbZ8n89e",
        "colab_type": "text"
      },
      "source": [
        "#SelectKBest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv3KcnDmTe9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "#É preciso fazer one hot encoding antes de fazer o one hot enconding do Keras (por que deus???)\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "le = LabelEncoder() \n",
        "df_concat[0]= le.fit_transform(df_concat[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOnHAAcGHPUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_values = df_concat.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "print(df_target)\n",
        "df_concat_new = SelectKBest(mutual_info_classif, k=75).fit_transform(df_values, df_target)\n",
        "df_concat_kbest = pd.DataFrame(df_concat_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXWkrSn3knnl",
        "colab_type": "text"
      },
      "source": [
        "#Matriz confusão RandomForest com SelectKBest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXFYPVR2kseu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_values = df_concat_kbest.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Ezf7DHlOVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_forest  = RandomForestClassifier()\n",
        "random_forest = random_forest.fit(X_train, y_train)\n",
        "y_predit = random_forest.predict(X_test)\n",
        "cm = confusion_matrix(y_test,y_predit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9lW05kvQv9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "03e182dc-08e0-495d-8a72-fe150f8238b7"
      },
      "source": [
        "index = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "columns = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "cm_matrix = confusion_matrix(y_test,y_predit)\n",
        "cm = [[j/sum(i) for j in i] for i in cm] #transformando em porcentagem \n",
        "cm = pd.DataFrame(cm,index,columns)                      \n",
        "plt.figure(figsize=(10,5))  \n",
        "sns.heatmap(cm, annot=True,fmt=\".2%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f953326a860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAEvCAYAAAAKDcjfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvPGCh3POOefyxgsezjnnnMub/wdeAr6Q51zwSAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq2bcW76xkSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#precisão\n",
        "precisao_inativo = cm_matrix[0][0]/(cm_matrix[0][0]+cm_matrix[0][1]+cm_matrix[0][2])\n",
        "precisao_moderada = cm_matrix[1][1]/(cm_matrix[1][0]+cm_matrix[1][1]+cm_matrix[1][2])\n",
        "precisao_vigorosa = cm_matrix[2][2]/(cm_matrix[2][0]+cm_matrix[2][1]+cm_matrix[2][2])\n",
        "#Sensibilidade\n",
        "sensibilidade_inativo = cm_matrix[0][0]/(cm_matrix[0][0]+cm_matrix[1][0]+cm_matrix[2][0])\n",
        "sensibilidade_moderada = cm_matrix[1][1]/(cm_matrix[0][1]+cm_matrix[1][1]+cm_matrix[2][1])\n",
        "sensibilidade_vigorosa = cm_matrix[2][2]/(cm_matrix[0][2]+cm_matrix[1][2]+cm_matrix[2][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gj-kMvf9751",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y2lemr-ezfH",
        "colab_type": "text"
      },
      "source": [
        "#Random Forest KFold com SelectKBest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDcgX3AiezFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83006b4a-51ac-421e-c58e-b8602ee58622"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "df_values = df_concat_kbest.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "scores = cross_val_score(RandomForestClassifier(n_estimators=100,max_depth=10,random_state=1,criterion='entropy'), df_values, df_target, cv=49,scoring=\"accuracy\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "vector_plot.append(scores.mean())\n",
        "vector_plot_error.append(scores.std())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.82 (+/- 0.08)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfox8ki4mFSn",
        "colab_type": "text"
      },
      "source": [
        "#Matriz Confusão da Rede Neural com SelectKBest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8dLvoi_mK9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create first network with Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from keras.utils import to_categorical\n",
        "import keras\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "#One hot encoding\n",
        "df_values = df_concat_kbest.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "#print(df_target)\n",
        "df_target = to_categorical(df_target)# split into input (X) and output (Y) variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, random_state=0)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(92, input_dim=74, init= 'uniform', activation='sigmoid'))\n",
        "model.add(Dense(4,activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='poisson',optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, nb_epoch=50, batch_size=64, validation_data = (X_test,y_test))\n",
        "# evaluate the model\n",
        "#scores = model.evaluate(X, Y)\n",
        "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6PM_eOGqTCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict_classes(X_test)\n",
        "from numpy import argmax\n",
        "y_test_labled = []\n",
        "for i in range (len(y_test)):\n",
        "  y_test_labled.append(argmax(y_test[i]))\n",
        "cm = confusion_matrix(y_test_labled,y_predit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdM0yprcmuVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "columns = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "cm_matrix = confusion_matrix(y_test_labled,y_predit)\n",
        "cm = [[j/sum(i) for j in i] for i in cm] #transformando em porcentagem \n",
        "cm = pd.DataFrame(cm,index,columns)                      \n",
        "plt.figure(figsize=(10,5))  \n",
        "sns.heatmap(cm, annot=True,fmt=\".2%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqVDhqVHfV6-",
        "colab_type": "text"
      },
      "source": [
        "#Rede neural com SelecKBest e cross-validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpafjkeFfUdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from itertools import product\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Create first network with Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from keras.utils.np_utils import to_categorical\n",
        "import keras\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "#Validação cruzada separando por indivíduo -- Sugestão da equipe MJV\n",
        "groups = df_concat[92] #ids\n",
        "df_values = df_concat_kbest.drop(columns=0)\n",
        "df_values_np =df_values.to_numpy()\n",
        "df_target = df_concat[0]\n",
        "\n",
        "gkf = GroupKFold(n_splits=49)\n",
        "cvscores = []\n",
        "for train, test in gkf.split(df_values, df_target, groups=groups):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(92, input_dim=74, init= 'uniform', activation='sigmoid'))\n",
        "  model.add(Dense(4,activation='softmax'))\n",
        "  df_target_categorial = to_categorical(df_target)# split into input (X) and output (Y) variables\n",
        "  print(len(df_target_categorial[0]))\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(loss='poisson',optimizer='adam', metrics=['accuracy'])\n",
        "  # Fit the model\n",
        "  model.fit(df_values_np[train], df_target_categorial[train], nb_epoch=50, batch_size=64)\n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(df_values_np[test], df_target_categorial[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]))\n",
        "  cvscores.append(scores[1] )\n",
        "\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
        "vector_plot.append(numpy.mean(cvscores))\n",
        "vector_plot_error.append(numpy.std(cvscores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQvr2d3ioBfD",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd9D7LuQkzQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "587c1458-23a6-4adc-c6cc-ef4aa1d7aa62"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "le = LabelEncoder() \n",
        "df_concat[0]= le.fit_transform(df_concat[0])\n",
        "df_values = df_concat.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "#df_simulation[0]= le.fit_transform(df_simulation[0])\n",
        "#df_values = df_simulation.drop(columns=0)\n",
        "#df_target_simulation = df_simulation[0]\n",
        "\n",
        "pca = PCA(n_components = 16,svd_solver = 'full')\n",
        "pca.fit(df_concat)\n",
        "df_concat_pca = pca.transform(df_concat)\n",
        "df_concat_pca = pd.DataFrame(df_concat_pca)\n",
        "df_concat_pca.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-6.988113</td>\n",
              "      <td>24.819618</td>\n",
              "      <td>-8.519803</td>\n",
              "      <td>-2.960030</td>\n",
              "      <td>-6.852361</td>\n",
              "      <td>1.725999</td>\n",
              "      <td>0.542788</td>\n",
              "      <td>1.231086</td>\n",
              "      <td>1.713754</td>\n",
              "      <td>0.182250</td>\n",
              "      <td>1.327686</td>\n",
              "      <td>-0.766976</td>\n",
              "      <td>-0.032895</td>\n",
              "      <td>0.587803</td>\n",
              "      <td>0.232022</td>\n",
              "      <td>-0.506493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-22.057828</td>\n",
              "      <td>20.599936</td>\n",
              "      <td>-9.416219</td>\n",
              "      <td>-2.484835</td>\n",
              "      <td>-7.276228</td>\n",
              "      <td>-0.163177</td>\n",
              "      <td>-1.168720</td>\n",
              "      <td>1.631094</td>\n",
              "      <td>1.998170</td>\n",
              "      <td>0.028084</td>\n",
              "      <td>0.738263</td>\n",
              "      <td>-0.188410</td>\n",
              "      <td>0.444639</td>\n",
              "      <td>0.466810</td>\n",
              "      <td>0.140047</td>\n",
              "      <td>-0.628853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-19.978031</td>\n",
              "      <td>22.674513</td>\n",
              "      <td>-6.118909</td>\n",
              "      <td>-1.302239</td>\n",
              "      <td>-6.885103</td>\n",
              "      <td>0.688408</td>\n",
              "      <td>-1.079620</td>\n",
              "      <td>1.498444</td>\n",
              "      <td>2.083121</td>\n",
              "      <td>0.234037</td>\n",
              "      <td>0.753383</td>\n",
              "      <td>-0.441616</td>\n",
              "      <td>0.252893</td>\n",
              "      <td>0.513846</td>\n",
              "      <td>-0.020667</td>\n",
              "      <td>-0.414094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-15.020522</td>\n",
              "      <td>22.010286</td>\n",
              "      <td>-8.916870</td>\n",
              "      <td>-6.078782</td>\n",
              "      <td>-7.296095</td>\n",
              "      <td>1.932581</td>\n",
              "      <td>-0.390878</td>\n",
              "      <td>1.417879</td>\n",
              "      <td>1.600820</td>\n",
              "      <td>0.180724</td>\n",
              "      <td>1.047509</td>\n",
              "      <td>-0.563341</td>\n",
              "      <td>0.059793</td>\n",
              "      <td>0.509543</td>\n",
              "      <td>0.130020</td>\n",
              "      <td>-0.399357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-18.486451</td>\n",
              "      <td>21.251272</td>\n",
              "      <td>-11.549467</td>\n",
              "      <td>1.760540</td>\n",
              "      <td>-6.785127</td>\n",
              "      <td>1.204353</td>\n",
              "      <td>0.303330</td>\n",
              "      <td>1.173929</td>\n",
              "      <td>1.872158</td>\n",
              "      <td>0.126026</td>\n",
              "      <td>0.966464</td>\n",
              "      <td>-0.520213</td>\n",
              "      <td>-0.305534</td>\n",
              "      <td>0.639661</td>\n",
              "      <td>0.105778</td>\n",
              "      <td>-0.472268</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0          1          2   ...        13        14        15\n",
              "0  -6.988113  24.819618  -8.519803  ...  0.587803  0.232022 -0.506493\n",
              "1 -22.057828  20.599936  -9.416219  ...  0.466810  0.140047 -0.628853\n",
              "2 -19.978031  22.674513  -6.118909  ...  0.513846 -0.020667 -0.414094\n",
              "3 -15.020522  22.010286  -8.916870  ...  0.509543  0.130020 -0.399357\n",
              "4 -18.486451  21.251272 -11.549467  ...  0.639661  0.105778 -0.472268\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvdkoBITtkGY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "8748c158-e320-4e15-fdbf-b3358f343943"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(svd_solver='full', n_components = df_concat.shape[1])\n",
        "pca.fit(df_concat)\n",
        "\n",
        "pca_sum_score = [(round(sum(pca.explained_variance_ratio_[0:n])*100,2)) for n in range(pca.n_components+1)]\n",
        "\n",
        "fig4, ax4 = plt.subplots(1, 2, figsize=(10, 6), dpi= 80, facecolor='w', edgecolor='k', sharey=True)\n",
        "fig4.suptitle('PCA Score') # or plt.suptitle('Main title')\n",
        "ax4[0].plot(pca_sum_score)\n",
        "ax4[0].title.set_text('Simple plot')\n",
        "ax4[0].set_xlabel('n_components')\n",
        "ax4[0].set_ylabel('Score')\n",
        "ax4[0].grid(color='k', linestyle=':', linewidth=1)\n",
        "\n",
        "ax4[1].bar(np.arange(pca.n_components+1),pca_sum_score)\n",
        "ax4[1].title.set_text('Bar plot')\n",
        "ax4[1].set_xlabel('n_components')\n",
        "\n",
        "print(pca_sum_score)\n",
        "\n",
        "pca_threshold = [50,75,90,95,97]\n",
        "\n",
        "for threshold in pca_threshold:\n",
        "    print(\"PCA Score is {} using {} components\".format(min(pca_sum_score, key=lambda x:abs(x-threshold)),pca_sum_score.index(min(pca_sum_score, key=lambda x:abs(x-threshold)))))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 47.1, 66.6, 82.32, 92.6, 95.55, 97.54, 98.57, 99.49, 99.62, 99.72, 99.78, 99.82, 99.86, 99.89, 99.91, 99.92, 99.93, 99.95, 99.96, 99.96, 99.97, 99.98, 99.98, 99.98, 99.99, 99.99, 99.99, 99.99, 99.99, 99.99, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "PCA Score is 47.1 using 1 components\n",
            "PCA Score is 82.32 using 3 components\n",
            "PCA Score is 92.6 using 4 components\n",
            "PCA Score is 95.55 using 5 components\n",
            "PCA Score is 97.54 using 6 components\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqcAAAHLCAYAAAD1OzG4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVny9RgMfTesAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 800x480 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6_vjW2Lm-Xe",
        "colab_type": "text"
      },
      "source": [
        "#Random forest com PCA e cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zPsx2GenTN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "f605db80-7d83-49ff-ef7f-c97c978d7e31"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "df_values = df_concat_pca.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "scores = cross_val_score(RandomForestClassifier(n_estimators=100,max_depth=10,random_state=1,criterion='entropy'), df_values, df_target, cv=10,scoring=\"accuracy\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "vector_plot.append(scores.mean())\n",
        "vector_plot_error.append(scores.std())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.94 (+/- 0.02)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-227994df9f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %0.2f (+/- %0.2f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvector_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvector_plot_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vector_plot' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtvsLvdMngX3",
        "colab_type": "text"
      },
      "source": [
        "#Matriz Confusão da RandomForest com PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsbvXivdnqaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "5f2aed75-fd12-4b4b-8bc7-2e0ebb256f6b"
      },
      "source": [
        "df_values = df_concat_pca.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, random_state=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3bdeea5f8776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_concat_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MufN5YKnu2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "4a2f7513-f0bd-4455-f452-b667e729b8cc"
      },
      "source": [
        "random_forest  = RandomForestClassifier()\n",
        "random_forest = random_forest.fit(X_train, y_train)\n",
        "y_predit = random_forest.predict(X_test)\n",
        "cm = confusion_matrix(y_test,y_predit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8a60ac48f6ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_forest\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_predit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_predit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlnZTij2nW-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "columns = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "cm_matrix = confusion_matrix(y_test,y_predit)\n",
        "cm = [[j/sum(i) for j in i] for i in cm] #transformando em porcentagem \n",
        "cm = pd.DataFrame(cm,index,columns)                      \n",
        "plt.figure(figsize=(10,5))  \n",
        "sns.heatmap(cm, annot=True,fmt=\".2%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZVh7DIoIr8",
        "colab_type": "text"
      },
      "source": [
        "#Matriz Confusão da Rede Neural com PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucasCLyboITy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67933c93-3a69-47cf-8646-1f88bee9b5b9"
      },
      "source": [
        "# Create first network with Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from keras.utils import to_categorical\n",
        "import keras\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "#One hot encoding\n",
        "df_values = df_concat_pca.drop(columns=0)\n",
        "df_target = df_concat[0]\n",
        "#print(df_target)\n",
        "df_target = to_categorical(df_target)# split into input (X) and output (Y) variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, random_state=0)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(92, input_dim=15, init= 'uniform', activation='sigmoid'))\n",
        "model.add(Dense(4,activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='poisson',optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, nb_epoch=50, batch_size=64, validation_data = (X_test,y_test))\n",
        "# evaluate the model\n",
        "#scores = model.evaluate(X, Y)\n",
        "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(92, input_dim=15, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 6094 samples, validate on 2032 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "6094/6094 [==============================] - 1s 122us/step - loss: 0.5140 - acc: 0.5760 - val_loss: 0.4658 - val_acc: 0.7023\n",
            "Epoch 2/50\n",
            "6094/6094 [==============================] - 0s 26us/step - loss: 0.4441 - acc: 0.7383 - val_loss: 0.4122 - val_acc: 0.8066\n",
            "Epoch 3/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.3961 - acc: 0.8310 - val_loss: 0.3709 - val_acc: 0.8652\n",
            "Epoch 4/50\n",
            "6094/6094 [==============================] - 0s 26us/step - loss: 0.3598 - acc: 0.8823 - val_loss: 0.3412 - val_acc: 0.9154\n",
            "Epoch 5/50\n",
            "6094/6094 [==============================] - 0s 23us/step - loss: 0.3344 - acc: 0.9189 - val_loss: 0.3218 - val_acc: 0.9434\n",
            "Epoch 6/50\n",
            "6094/6094 [==============================] - 0s 28us/step - loss: 0.3171 - acc: 0.9393 - val_loss: 0.3073 - val_acc: 0.9582\n",
            "Epoch 7/50\n",
            "6094/6094 [==============================] - 0s 23us/step - loss: 0.3053 - acc: 0.9529 - val_loss: 0.2975 - val_acc: 0.9675\n",
            "Epoch 8/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2965 - acc: 0.9621 - val_loss: 0.2908 - val_acc: 0.9705\n",
            "Epoch 9/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2901 - acc: 0.9652 - val_loss: 0.2853 - val_acc: 0.9734\n",
            "Epoch 10/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2852 - acc: 0.9721 - val_loss: 0.2813 - val_acc: 0.9769\n",
            "Epoch 11/50\n",
            "6094/6094 [==============================] - 0s 27us/step - loss: 0.2812 - acc: 0.9747 - val_loss: 0.2778 - val_acc: 0.9764\n",
            "Epoch 12/50\n",
            "6094/6094 [==============================] - 0s 28us/step - loss: 0.2779 - acc: 0.9769 - val_loss: 0.2749 - val_acc: 0.9813\n",
            "Epoch 13/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2751 - acc: 0.9793 - val_loss: 0.2725 - val_acc: 0.9808\n",
            "Epoch 14/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2729 - acc: 0.9815 - val_loss: 0.2710 - val_acc: 0.9838\n",
            "Epoch 15/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2708 - acc: 0.9826 - val_loss: 0.2691 - val_acc: 0.9857\n",
            "Epoch 16/50\n",
            "6094/6094 [==============================] - 0s 26us/step - loss: 0.2690 - acc: 0.9846 - val_loss: 0.2672 - val_acc: 0.9882\n",
            "Epoch 17/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2674 - acc: 0.9865 - val_loss: 0.2658 - val_acc: 0.9916\n",
            "Epoch 18/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2661 - acc: 0.9883 - val_loss: 0.2646 - val_acc: 0.9911\n",
            "Epoch 19/50\n",
            "6094/6094 [==============================] - 0s 27us/step - loss: 0.2647 - acc: 0.9893 - val_loss: 0.2635 - val_acc: 0.9916\n",
            "Epoch 20/50\n",
            "6094/6094 [==============================] - 0s 26us/step - loss: 0.2636 - acc: 0.9906 - val_loss: 0.2626 - val_acc: 0.9921\n",
            "Epoch 21/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2627 - acc: 0.9913 - val_loss: 0.2619 - val_acc: 0.9936\n",
            "Epoch 22/50\n",
            "6094/6094 [==============================] - 0s 26us/step - loss: 0.2617 - acc: 0.9931 - val_loss: 0.2610 - val_acc: 0.9921\n",
            "Epoch 23/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2610 - acc: 0.9925 - val_loss: 0.2605 - val_acc: 0.9926\n",
            "Epoch 24/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2602 - acc: 0.9947 - val_loss: 0.2603 - val_acc: 0.9931\n",
            "Epoch 25/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2599 - acc: 0.9943 - val_loss: 0.2594 - val_acc: 0.9926\n",
            "Epoch 26/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2589 - acc: 0.9956 - val_loss: 0.2589 - val_acc: 0.9941\n",
            "Epoch 27/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2584 - acc: 0.9956 - val_loss: 0.2584 - val_acc: 0.9936\n",
            "Epoch 28/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2578 - acc: 0.9962 - val_loss: 0.2580 - val_acc: 0.9946\n",
            "Epoch 29/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2574 - acc: 0.9962 - val_loss: 0.2577 - val_acc: 0.9941\n",
            "Epoch 30/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2569 - acc: 0.9970 - val_loss: 0.2575 - val_acc: 0.9951\n",
            "Epoch 31/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2565 - acc: 0.9969 - val_loss: 0.2570 - val_acc: 0.9946\n",
            "Epoch 32/50\n",
            "6094/6094 [==============================] - 0s 26us/step - loss: 0.2561 - acc: 0.9967 - val_loss: 0.2568 - val_acc: 0.9951\n",
            "Epoch 33/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2558 - acc: 0.9972 - val_loss: 0.2567 - val_acc: 0.9951\n",
            "Epoch 34/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2555 - acc: 0.9975 - val_loss: 0.2562 - val_acc: 0.9951\n",
            "Epoch 35/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2552 - acc: 0.9982 - val_loss: 0.2563 - val_acc: 0.9956\n",
            "Epoch 36/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2552 - acc: 0.9977 - val_loss: 0.2559 - val_acc: 0.9951\n",
            "Epoch 37/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2548 - acc: 0.9979 - val_loss: 0.2557 - val_acc: 0.9951\n",
            "Epoch 38/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2544 - acc: 0.9984 - val_loss: 0.2555 - val_acc: 0.9951\n",
            "Epoch 39/50\n",
            "6094/6094 [==============================] - 0s 25us/step - loss: 0.2542 - acc: 0.9985 - val_loss: 0.2555 - val_acc: 0.9956\n",
            "Epoch 40/50\n",
            "6094/6094 [==============================] - 0s 26us/step - loss: 0.2541 - acc: 0.9984 - val_loss: 0.2553 - val_acc: 0.9956\n",
            "Epoch 41/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2538 - acc: 0.9982 - val_loss: 0.2550 - val_acc: 0.9961\n",
            "Epoch 42/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2537 - acc: 0.9985 - val_loss: 0.2551 - val_acc: 0.9956\n",
            "Epoch 43/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2544 - acc: 0.9969 - val_loss: 0.2549 - val_acc: 0.9956\n",
            "Epoch 44/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2534 - acc: 0.9985 - val_loss: 0.2549 - val_acc: 0.9961\n",
            "Epoch 45/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2532 - acc: 0.9984 - val_loss: 0.2547 - val_acc: 0.9961\n",
            "Epoch 46/50\n",
            "6094/6094 [==============================] - 0s 22us/step - loss: 0.2536 - acc: 0.9985 - val_loss: 0.2548 - val_acc: 0.9951\n",
            "Epoch 47/50\n",
            "6094/6094 [==============================] - 0s 22us/step - loss: 0.2529 - acc: 0.9987 - val_loss: 0.2545 - val_acc: 0.9956\n",
            "Epoch 48/50\n",
            "6094/6094 [==============================] - 0s 22us/step - loss: 0.2528 - acc: 0.9992 - val_loss: 0.2544 - val_acc: 0.9956\n",
            "Epoch 49/50\n",
            "6094/6094 [==============================] - 0s 23us/step - loss: 0.2527 - acc: 0.9990 - val_loss: 0.2544 - val_acc: 0.9961\n",
            "Epoch 50/50\n",
            "6094/6094 [==============================] - 0s 24us/step - loss: 0.2525 - acc: 0.9990 - val_loss: 0.2546 - val_acc: 0.9961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efca6ee9f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXujnNBPoSuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "8071add3-2c82-45d7-df58-e47fdd95b405"
      },
      "source": [
        "y_predict = model.predict_classes(X_test)\n",
        "from numpy import argmax\n",
        "y_test_labled = []\n",
        "for i in range (len(y_test)):\n",
        "  y_test_labled.append(argmax(y_test[i]))\n",
        "cm = confusion_matrix(y_test_labled,y_predit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-87dae072fef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0my_test_labled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_labled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_predit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZtQTfIoZWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "c9570e68-2aee-4902-f7fe-4b0cd0261583"
      },
      "source": [
        "index = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "columns = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "cm_matrix = confusion_matrix(y_test_labled,y_predit)\n",
        "cm = [[j/sum(i) for j in i] for i in cm] #transformando em porcentagem \n",
        "cm = pd.DataFrame(cm,index,columns)                      \n",
        "plt.figure(figsize=(10,5))  \n",
        "sns.heatmap(cm, annot=True,fmt=\".2%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b5347b72d06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'CAMINHADA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'INATIVO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MODERADA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'VIGOROSA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'CAMINHADA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'INATIVO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MODERADA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'VIGOROSA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcm_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_labled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_predit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#transformando em porcentagem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3NUO15X-CiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#precisão\n",
        "precisao_inativo = cm_matrix[0][0]/(cm_matrix[0][0]+cm_matrix[0][1]+cm_matrix[0][2])\n",
        "precisao_moderada = cm_matrix[1][1]/(cm_matrix[1][0]+cm_matrix[1][1]+cm_matrix[1][2])\n",
        "precisao_vigorosa = cm_matrix[2][2]/(cm_matrix[2][0]+cm_matrix[2][1]+cm_matrix[2][2])\n",
        "#Sensibilidade\n",
        "sensibilidade_inativo = cm_matrix[0][0]/(cm_matrix[0][0]+cm_matrix[1][0]+cm_matrix[2][0])\n",
        "sensibilidade_moderada = cm_matrix[1][1]/(cm_matrix[0][1]+cm_matrix[1][1]+cm_matrix[2][1])\n",
        "sensibilidade_vigorosa = cm_matrix[2][2]/(cm_matrix[0][2]+cm_matrix[1][2]+cm_matrix[2][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnCq7ht-irll",
        "colab_type": "text"
      },
      "source": [
        "#Rede Neural com PCA e Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfViHdWVi2qT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b41ce4a-1d0b-445f-b74a-63a9ad817d8b"
      },
      "source": [
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from itertools import product\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Create first network with Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from keras.utils.np_utils import to_categorical\n",
        "import keras\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "df_values = df_concat_pca.drop(columns=0)\n",
        "df_values_np =df_values.to_numpy()\n",
        "df_target = df_concat[0]\n",
        "#Validação cruzada separando por indivíduo -- Sugestão da equipe MJV\n",
        "groups = df_concat[92] #ids\n",
        "df_values = df_concat_pca.drop(columns=0)\n",
        "df_values_np =df_values.to_numpy()\n",
        "df_target = df_concat[0]\n",
        "\n",
        "gkf = GroupKFold(n_splits=10)\n",
        "cvscores = []\n",
        "for train, test in gkf.split(df_values, df_target, groups=groups):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(92, input_dim=15, init= 'uniform', activation='sigmoid'))\n",
        "  model.add(Dense(4,activation='softmax'))\n",
        "  df_target_categorial = to_categorical(df_target)\n",
        "  print(len(df_target_categorial[0]))\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(loss='poisson',optimizer='adam', metrics=['accuracy'])\n",
        "  # Fit the model\n",
        "  model.fit(df_values_np[train], df_target_categorial[train], nb_epoch=50, batch_size=64)\n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(df_values_np[test], df_target_categorial[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]))\n",
        "  cvscores.append(scores[1])\n",
        "\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
        "vector_plot.append(numpy.mean(cvscores))\n",
        "vector_plot_error.append(numpy.std(cvscores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(92, input_dim=15, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "Epoch 1/50\n",
            "7336/7336 [==============================] - 0s 44us/step - loss: 0.5021 - acc: 0.6076\n",
            "Epoch 2/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.4283 - acc: 0.7627\n",
            "Epoch 3/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.3780 - acc: 0.8555\n",
            "Epoch 4/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.3422 - acc: 0.9122\n",
            "Epoch 5/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.3190 - acc: 0.9426\n",
            "Epoch 6/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.3040 - acc: 0.9565\n",
            "Epoch 7/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2941 - acc: 0.9643\n",
            "Epoch 8/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2871 - acc: 0.9712\n",
            "Epoch 9/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2820 - acc: 0.9748\n",
            "Epoch 10/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2782 - acc: 0.9759\n",
            "Epoch 11/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2748 - acc: 0.9793\n",
            "Epoch 12/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2720 - acc: 0.9821\n",
            "Epoch 13/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2698 - acc: 0.9846\n",
            "Epoch 14/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2678 - acc: 0.9868\n",
            "Epoch 15/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2663 - acc: 0.9881\n",
            "Epoch 16/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2648 - acc: 0.9900\n",
            "Epoch 17/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2635 - acc: 0.9911\n",
            "Epoch 18/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2624 - acc: 0.9914\n",
            "Epoch 19/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2615 - acc: 0.9925\n",
            "Epoch 20/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2607 - acc: 0.9937\n",
            "Epoch 21/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2597 - acc: 0.9945\n",
            "Epoch 22/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2590 - acc: 0.9951\n",
            "Epoch 23/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2584 - acc: 0.9956\n",
            "Epoch 24/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2579 - acc: 0.9952\n",
            "Epoch 25/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2573 - acc: 0.9962\n",
            "Epoch 26/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2569 - acc: 0.9963\n",
            "Epoch 27/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2565 - acc: 0.9969\n",
            "Epoch 28/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2561 - acc: 0.9966\n",
            "Epoch 29/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2557 - acc: 0.9971\n",
            "Epoch 30/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2555 - acc: 0.9971\n",
            "Epoch 31/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2552 - acc: 0.9975\n",
            "Epoch 32/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2550 - acc: 0.9974\n",
            "Epoch 33/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2547 - acc: 0.9978\n",
            "Epoch 34/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2545 - acc: 0.9977\n",
            "Epoch 35/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2542 - acc: 0.9975\n",
            "Epoch 36/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2540 - acc: 0.9982\n",
            "Epoch 37/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2538 - acc: 0.9982\n",
            "Epoch 38/50\n",
            "7336/7336 [==============================] - 0s 18us/step - loss: 0.2535 - acc: 0.9985\n",
            "Epoch 39/50\n",
            "7336/7336 [==============================] - 0s 21us/step - loss: 0.2534 - acc: 0.9984\n",
            "Epoch 40/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2533 - acc: 0.9985\n",
            "Epoch 41/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2531 - acc: 0.9985\n",
            "Epoch 42/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2530 - acc: 0.9984\n",
            "Epoch 43/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2528 - acc: 0.9985\n",
            "Epoch 44/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2527 - acc: 0.9986\n",
            "Epoch 45/50\n",
            "7336/7336 [==============================] - 0s 20us/step - loss: 0.2526 - acc: 0.9985\n",
            "Epoch 46/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2525 - acc: 0.9986\n",
            "Epoch 47/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2524 - acc: 0.9989\n",
            "Epoch 48/50\n",
            "7336/7336 [==============================] - 0s 18us/step - loss: 0.2523 - acc: 0.9989\n",
            "Epoch 49/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2522 - acc: 0.9992\n",
            "Epoch 50/50\n",
            "7336/7336 [==============================] - 0s 19us/step - loss: 0.2522 - acc: 0.9985\n",
            "acc: 1.00%\n",
            "4\n",
            "Epoch 1/50\n",
            "7337/7337 [==============================] - 0s 51us/step - loss: 0.5015 - acc: 0.6038\n",
            "Epoch 2/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.4234 - acc: 0.7804\n",
            "Epoch 3/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3746 - acc: 0.8558\n",
            "Epoch 4/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3409 - acc: 0.9092\n",
            "Epoch 5/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.3186 - acc: 0.9391\n",
            "Epoch 6/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3041 - acc: 0.9533\n",
            "Epoch 7/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2943 - acc: 0.9635\n",
            "Epoch 8/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2871 - acc: 0.9710\n",
            "Epoch 9/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2820 - acc: 0.9744\n",
            "Epoch 10/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.2781 - acc: 0.9767\n",
            "Epoch 11/50\n",
            "7337/7337 [==============================] - 0s 25us/step - loss: 0.2746 - acc: 0.9805\n",
            "Epoch 12/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2720 - acc: 0.9826\n",
            "Epoch 13/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2697 - acc: 0.9858\n",
            "Epoch 14/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2678 - acc: 0.9861\n",
            "Epoch 15/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2660 - acc: 0.9884\n",
            "Epoch 16/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2646 - acc: 0.9902\n",
            "Epoch 17/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2633 - acc: 0.9914\n",
            "Epoch 18/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2623 - acc: 0.9925\n",
            "Epoch 19/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2612 - acc: 0.9930\n",
            "Epoch 20/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2604 - acc: 0.9926\n",
            "Epoch 21/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2596 - acc: 0.9939\n",
            "Epoch 22/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2589 - acc: 0.9947\n",
            "Epoch 23/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2582 - acc: 0.9954\n",
            "Epoch 24/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2577 - acc: 0.9955\n",
            "Epoch 25/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2571 - acc: 0.9965\n",
            "Epoch 26/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2568 - acc: 0.9966\n",
            "Epoch 27/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2563 - acc: 0.9967\n",
            "Epoch 28/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2559 - acc: 0.9973\n",
            "Epoch 29/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2556 - acc: 0.9974\n",
            "Epoch 30/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2552 - acc: 0.9969\n",
            "Epoch 31/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2550 - acc: 0.9974\n",
            "Epoch 32/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2547 - acc: 0.9975\n",
            "Epoch 33/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2545 - acc: 0.9977\n",
            "Epoch 34/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2542 - acc: 0.9980\n",
            "Epoch 35/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2540 - acc: 0.9981\n",
            "Epoch 36/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2538 - acc: 0.9982\n",
            "Epoch 37/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2536 - acc: 0.9980\n",
            "Epoch 38/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2534 - acc: 0.9982\n",
            "Epoch 39/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2537 - acc: 0.9982\n",
            "Epoch 40/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2532 - acc: 0.9982\n",
            "Epoch 41/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2530 - acc: 0.9986\n",
            "Epoch 42/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2528 - acc: 0.9986\n",
            "Epoch 43/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2527 - acc: 0.9985\n",
            "Epoch 44/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2526 - acc: 0.9988\n",
            "Epoch 45/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2525 - acc: 0.9985\n",
            "Epoch 46/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2523 - acc: 0.9988\n",
            "Epoch 47/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2523 - acc: 0.9988\n",
            "Epoch 48/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2521 - acc: 0.9986\n",
            "Epoch 49/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2520 - acc: 0.9989\n",
            "Epoch 50/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.2519 - acc: 0.9990\n",
            "acc: 1.00%\n",
            "4\n",
            "Epoch 1/50\n",
            "7337/7337 [==============================] - 0s 57us/step - loss: 0.5194 - acc: 0.5817\n",
            "Epoch 2/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.4308 - acc: 0.7680\n",
            "Epoch 3/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3853 - acc: 0.8422\n",
            "Epoch 4/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3518 - acc: 0.8996\n",
            "Epoch 5/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3282 - acc: 0.9290\n",
            "Epoch 6/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.3119 - acc: 0.9493\n",
            "Epoch 7/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.3006 - acc: 0.9598\n",
            "Epoch 8/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2926 - acc: 0.9655\n",
            "Epoch 9/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2867 - acc: 0.9700\n",
            "Epoch 10/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2820 - acc: 0.9744\n",
            "Epoch 11/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2783 - acc: 0.9770\n",
            "Epoch 12/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2753 - acc: 0.9791\n",
            "Epoch 13/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2729 - acc: 0.9819\n",
            "Epoch 14/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2706 - acc: 0.9835\n",
            "Epoch 15/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2687 - acc: 0.9864\n",
            "Epoch 16/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2672 - acc: 0.9884\n",
            "Epoch 17/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2657 - acc: 0.9905\n",
            "Epoch 18/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2644 - acc: 0.9918\n",
            "Epoch 19/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2632 - acc: 0.9922\n",
            "Epoch 20/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2621 - acc: 0.9926\n",
            "Epoch 21/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2613 - acc: 0.9933\n",
            "Epoch 22/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2603 - acc: 0.9943\n",
            "Epoch 23/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2596 - acc: 0.9944\n",
            "Epoch 24/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2590 - acc: 0.9945\n",
            "Epoch 25/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2584 - acc: 0.9951\n",
            "Epoch 26/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2578 - acc: 0.9962\n",
            "Epoch 27/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2573 - acc: 0.9966\n",
            "Epoch 28/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.2569 - acc: 0.9966\n",
            "Epoch 29/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2564 - acc: 0.9967\n",
            "Epoch 30/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2561 - acc: 0.9974\n",
            "Epoch 31/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2557 - acc: 0.9973\n",
            "Epoch 32/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2554 - acc: 0.9974\n",
            "Epoch 33/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2551 - acc: 0.9977\n",
            "Epoch 34/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2549 - acc: 0.9973\n",
            "Epoch 35/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2546 - acc: 0.9981\n",
            "Epoch 36/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2544 - acc: 0.9981\n",
            "Epoch 37/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2541 - acc: 0.9982\n",
            "Epoch 38/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2540 - acc: 0.9985\n",
            "Epoch 39/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2537 - acc: 0.9981\n",
            "Epoch 40/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2536 - acc: 0.9984\n",
            "Epoch 41/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2533 - acc: 0.9984\n",
            "Epoch 42/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2532 - acc: 0.9984\n",
            "Epoch 43/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2530 - acc: 0.9984\n",
            "Epoch 44/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.2529 - acc: 0.9986\n",
            "Epoch 45/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2528 - acc: 0.9985\n",
            "Epoch 46/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2527 - acc: 0.9984\n",
            "Epoch 47/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2526 - acc: 0.9988\n",
            "Epoch 48/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2525 - acc: 0.9984\n",
            "Epoch 49/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2523 - acc: 0.9986\n",
            "Epoch 50/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2522 - acc: 0.9986\n",
            "acc: 1.00%\n",
            "4\n",
            "Epoch 1/50\n",
            "7337/7337 [==============================] - 0s 67us/step - loss: 0.5005 - acc: 0.6135\n",
            "Epoch 2/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.4210 - acc: 0.7863\n",
            "Epoch 3/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3705 - acc: 0.8678\n",
            "Epoch 4/50\n",
            "7337/7337 [==============================] - 0s 19us/step - loss: 0.3373 - acc: 0.9182\n",
            "Epoch 5/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.3161 - acc: 0.9430\n",
            "Epoch 6/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.3025 - acc: 0.9565\n",
            "Epoch 7/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2933 - acc: 0.9650\n",
            "Epoch 8/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2867 - acc: 0.9699\n",
            "Epoch 9/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2817 - acc: 0.9731\n",
            "Epoch 10/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2778 - acc: 0.9766\n",
            "Epoch 11/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2746 - acc: 0.9794\n",
            "Epoch 12/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2720 - acc: 0.9816\n",
            "Epoch 13/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2698 - acc: 0.9835\n",
            "Epoch 14/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2679 - acc: 0.9872\n",
            "Epoch 15/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2662 - acc: 0.9880\n",
            "Epoch 16/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2647 - acc: 0.9894\n",
            "Epoch 17/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2636 - acc: 0.9907\n",
            "Epoch 18/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2623 - acc: 0.9925\n",
            "Epoch 19/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2614 - acc: 0.9925\n",
            "Epoch 20/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2605 - acc: 0.9937\n",
            "Epoch 21/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2597 - acc: 0.9948\n",
            "Epoch 22/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2590 - acc: 0.9951\n",
            "Epoch 23/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2585 - acc: 0.9952\n",
            "Epoch 24/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2578 - acc: 0.9962\n",
            "Epoch 25/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2572 - acc: 0.9959\n",
            "Epoch 26/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2567 - acc: 0.9967\n",
            "Epoch 27/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2564 - acc: 0.9963\n",
            "Epoch 28/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2560 - acc: 0.9973\n",
            "Epoch 29/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2557 - acc: 0.9974\n",
            "Epoch 30/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2554 - acc: 0.9969\n",
            "Epoch 31/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2551 - acc: 0.9974\n",
            "Epoch 32/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2547 - acc: 0.9980\n",
            "Epoch 33/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2546 - acc: 0.9978\n",
            "Epoch 34/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2543 - acc: 0.9975\n",
            "Epoch 35/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2541 - acc: 0.9980\n",
            "Epoch 36/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2539 - acc: 0.9981\n",
            "Epoch 37/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2537 - acc: 0.9978\n",
            "Epoch 38/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2535 - acc: 0.9984\n",
            "Epoch 39/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2533 - acc: 0.9982\n",
            "Epoch 40/50\n",
            "7337/7337 [==============================] - 0s 23us/step - loss: 0.2531 - acc: 0.9982\n",
            "Epoch 41/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2530 - acc: 0.9988\n",
            "Epoch 42/50\n",
            "7337/7337 [==============================] - 0s 22us/step - loss: 0.2529 - acc: 0.9985\n",
            "Epoch 43/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2528 - acc: 0.9985\n",
            "Epoch 44/50\n",
            "7337/7337 [==============================] - 0s 24us/step - loss: 0.2526 - acc: 0.9988\n",
            "Epoch 45/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2525 - acc: 0.9988\n",
            "Epoch 46/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2523 - acc: 0.9990\n",
            "Epoch 47/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2522 - acc: 0.9990\n",
            "Epoch 48/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2523 - acc: 0.9989\n",
            "Epoch 49/50\n",
            "7337/7337 [==============================] - 0s 20us/step - loss: 0.2521 - acc: 0.9990\n",
            "Epoch 50/50\n",
            "7337/7337 [==============================] - 0s 21us/step - loss: 0.2520 - acc: 0.9990\n",
            "acc: 1.00%\n",
            "4\n",
            "Epoch 1/50\n",
            "7374/7374 [==============================] - 1s 73us/step - loss: 0.5044 - acc: 0.6084\n",
            "Epoch 2/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.4277 - acc: 0.7704\n",
            "Epoch 3/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.3765 - acc: 0.8518\n",
            "Epoch 4/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.3416 - acc: 0.9109\n",
            "Epoch 5/50\n",
            "7374/7374 [==============================] - 0s 24us/step - loss: 0.3187 - acc: 0.9403\n",
            "Epoch 6/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.3042 - acc: 0.9561\n",
            "Epoch 7/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2942 - acc: 0.9643\n",
            "Epoch 8/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2872 - acc: 0.9700\n",
            "Epoch 9/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2820 - acc: 0.9752\n",
            "Epoch 10/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2779 - acc: 0.9778\n",
            "Epoch 11/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.2748 - acc: 0.9797\n",
            "Epoch 12/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2721 - acc: 0.9832\n",
            "Epoch 13/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2697 - acc: 0.9852\n",
            "Epoch 14/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2679 - acc: 0.9875\n",
            "Epoch 15/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2661 - acc: 0.9890\n",
            "Epoch 16/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2645 - acc: 0.9910\n",
            "Epoch 17/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2633 - acc: 0.9915\n",
            "Epoch 18/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2622 - acc: 0.9928\n",
            "Epoch 19/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2613 - acc: 0.9931\n",
            "Epoch 20/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.2603 - acc: 0.9935\n",
            "Epoch 21/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.2597 - acc: 0.9939\n",
            "Epoch 22/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2588 - acc: 0.9951\n",
            "Epoch 23/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2582 - acc: 0.9951\n",
            "Epoch 24/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2576 - acc: 0.9959\n",
            "Epoch 25/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2572 - acc: 0.9959\n",
            "Epoch 26/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2566 - acc: 0.9963\n",
            "Epoch 27/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2563 - acc: 0.9963\n",
            "Epoch 28/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2559 - acc: 0.9970\n",
            "Epoch 29/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2556 - acc: 0.9969\n",
            "Epoch 30/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2552 - acc: 0.9976\n",
            "Epoch 31/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2550 - acc: 0.9973\n",
            "Epoch 32/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2547 - acc: 0.9978\n",
            "Epoch 33/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2545 - acc: 0.9977\n",
            "Epoch 34/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2543 - acc: 0.9978\n",
            "Epoch 35/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.2540 - acc: 0.9980\n",
            "Epoch 36/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2538 - acc: 0.9980\n",
            "Epoch 37/50\n",
            "7374/7374 [==============================] - 0s 23us/step - loss: 0.2537 - acc: 0.9978\n",
            "Epoch 38/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2535 - acc: 0.9981\n",
            "Epoch 39/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2533 - acc: 0.9982\n",
            "Epoch 40/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.2532 - acc: 0.9984\n",
            "Epoch 41/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2530 - acc: 0.9985\n",
            "Epoch 42/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2529 - acc: 0.9981\n",
            "Epoch 43/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2528 - acc: 0.9985\n",
            "Epoch 44/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2527 - acc: 0.9988\n",
            "Epoch 45/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2525 - acc: 0.9988\n",
            "Epoch 46/50\n",
            "7374/7374 [==============================] - 0s 21us/step - loss: 0.2525 - acc: 0.9984\n",
            "Epoch 47/50\n",
            "7374/7374 [==============================] - 0s 23us/step - loss: 0.2524 - acc: 0.9988\n",
            "Epoch 48/50\n",
            "7374/7374 [==============================] - 0s 24us/step - loss: 0.2522 - acc: 0.9988\n",
            "Epoch 49/50\n",
            "7374/7374 [==============================] - 0s 22us/step - loss: 0.2522 - acc: 0.9989\n",
            "Epoch 50/50\n",
            "7374/7374 [==============================] - 0s 20us/step - loss: 0.2521 - acc: 0.9992\n",
            "acc: 1.00%\n",
            "4\n",
            "Epoch 1/50\n",
            "7379/7379 [==============================] - 1s 83us/step - loss: 0.5025 - acc: 0.6139\n",
            "Epoch 2/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.4201 - acc: 0.7870\n",
            "Epoch 3/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.3676 - acc: 0.8698\n",
            "Epoch 4/50\n",
            "7379/7379 [==============================] - 0s 21us/step - loss: 0.3336 - acc: 0.9241\n",
            "Epoch 5/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.3123 - acc: 0.9501\n",
            "Epoch 6/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2989 - acc: 0.9634\n",
            "Epoch 7/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2900 - acc: 0.9692\n",
            "Epoch 8/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2833 - acc: 0.9729\n",
            "Epoch 9/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2788 - acc: 0.9752\n",
            "Epoch 10/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2749 - acc: 0.9798\n",
            "Epoch 11/50\n",
            "7379/7379 [==============================] - 0s 24us/step - loss: 0.2720 - acc: 0.9818\n",
            "Epoch 12/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2695 - acc: 0.9848\n",
            "Epoch 13/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2675 - acc: 0.9875\n",
            "Epoch 14/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2658 - acc: 0.9882\n",
            "Epoch 15/50\n",
            "7379/7379 [==============================] - 0s 21us/step - loss: 0.2642 - acc: 0.9900\n",
            "Epoch 16/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2629 - acc: 0.9911\n",
            "Epoch 17/50\n",
            "7379/7379 [==============================] - 0s 21us/step - loss: 0.2619 - acc: 0.9925\n",
            "Epoch 18/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2609 - acc: 0.9928\n",
            "Epoch 19/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2600 - acc: 0.9935\n",
            "Epoch 20/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2592 - acc: 0.9940\n",
            "Epoch 21/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2586 - acc: 0.9942\n",
            "Epoch 22/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2579 - acc: 0.9953\n",
            "Epoch 23/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2573 - acc: 0.9961\n",
            "Epoch 24/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2568 - acc: 0.9969\n",
            "Epoch 25/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2568 - acc: 0.9958\n",
            "Epoch 26/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2560 - acc: 0.9969\n",
            "Epoch 27/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2556 - acc: 0.9972\n",
            "Epoch 28/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2553 - acc: 0.9972\n",
            "Epoch 29/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2550 - acc: 0.9978\n",
            "Epoch 30/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2548 - acc: 0.9977\n",
            "Epoch 31/50\n",
            "7379/7379 [==============================] - 0s 21us/step - loss: 0.2545 - acc: 0.9978\n",
            "Epoch 32/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2543 - acc: 0.9978\n",
            "Epoch 33/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2540 - acc: 0.9984\n",
            "Epoch 34/50\n",
            "7379/7379 [==============================] - 0s 18us/step - loss: 0.2538 - acc: 0.9981\n",
            "Epoch 35/50\n",
            "7379/7379 [==============================] - 0s 18us/step - loss: 0.2536 - acc: 0.9982\n",
            "Epoch 36/50\n",
            "7379/7379 [==============================] - 0s 21us/step - loss: 0.2535 - acc: 0.9981\n",
            "Epoch 37/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2533 - acc: 0.9984\n",
            "Epoch 38/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2531 - acc: 0.9986\n",
            "Epoch 39/50\n",
            "7379/7379 [==============================] - 0s 22us/step - loss: 0.2530 - acc: 0.9984\n",
            "Epoch 40/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2529 - acc: 0.9986\n",
            "Epoch 41/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2528 - acc: 0.9985\n",
            "Epoch 42/50\n",
            "7379/7379 [==============================] - 0s 21us/step - loss: 0.2527 - acc: 0.9985\n",
            "Epoch 43/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2525 - acc: 0.9989\n",
            "Epoch 44/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2525 - acc: 0.9989\n",
            "Epoch 45/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2523 - acc: 0.9989\n",
            "Epoch 46/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2522 - acc: 0.9989\n",
            "Epoch 47/50\n",
            "7379/7379 [==============================] - 0s 24us/step - loss: 0.2521 - acc: 0.9989\n",
            "Epoch 48/50\n",
            "7379/7379 [==============================] - 0s 19us/step - loss: 0.2520 - acc: 0.9989\n",
            "Epoch 49/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2519 - acc: 0.9991\n",
            "Epoch 50/50\n",
            "7379/7379 [==============================] - 0s 20us/step - loss: 0.2519 - acc: 0.9991\n",
            "acc: 0.99%\n",
            "4\n",
            "Epoch 1/50\n",
            "7263/7263 [==============================] - 1s 86us/step - loss: 0.5013 - acc: 0.6181\n",
            "Epoch 2/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.4220 - acc: 0.7779\n",
            "Epoch 3/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.3716 - acc: 0.8636\n",
            "Epoch 4/50\n",
            "7263/7263 [==============================] - 0s 24us/step - loss: 0.3375 - acc: 0.9197\n",
            "Epoch 5/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.3159 - acc: 0.9435\n",
            "Epoch 6/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.3018 - acc: 0.9566\n",
            "Epoch 7/50\n",
            "7263/7263 [==============================] - 0s 19us/step - loss: 0.2926 - acc: 0.9645\n",
            "Epoch 8/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2860 - acc: 0.9692\n",
            "Epoch 9/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2809 - acc: 0.9737\n",
            "Epoch 10/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2770 - acc: 0.9773\n",
            "Epoch 11/50\n",
            "7263/7263 [==============================] - 0s 22us/step - loss: 0.2739 - acc: 0.9809\n",
            "Epoch 12/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2713 - acc: 0.9835\n",
            "Epoch 13/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2692 - acc: 0.9858\n",
            "Epoch 14/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2672 - acc: 0.9886\n",
            "Epoch 15/50\n",
            "7263/7263 [==============================] - 0s 22us/step - loss: 0.2657 - acc: 0.9887\n",
            "Epoch 16/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2642 - acc: 0.9915\n",
            "Epoch 17/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2631 - acc: 0.9916\n",
            "Epoch 18/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2619 - acc: 0.9927\n",
            "Epoch 19/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2609 - acc: 0.9933\n",
            "Epoch 20/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2602 - acc: 0.9937\n",
            "Epoch 21/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2593 - acc: 0.9945\n",
            "Epoch 22/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2586 - acc: 0.9953\n",
            "Epoch 23/50\n",
            "7263/7263 [==============================] - 0s 24us/step - loss: 0.2580 - acc: 0.9957\n",
            "Epoch 24/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2575 - acc: 0.9960\n",
            "Epoch 25/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2571 - acc: 0.9957\n",
            "Epoch 26/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2565 - acc: 0.9970\n",
            "Epoch 27/50\n",
            "7263/7263 [==============================] - 0s 22us/step - loss: 0.2562 - acc: 0.9971\n",
            "Epoch 28/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2558 - acc: 0.9972\n",
            "Epoch 29/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2555 - acc: 0.9975\n",
            "Epoch 30/50\n",
            "7263/7263 [==============================] - 0s 19us/step - loss: 0.2551 - acc: 0.9975\n",
            "Epoch 31/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2549 - acc: 0.9978\n",
            "Epoch 32/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2546 - acc: 0.9978\n",
            "Epoch 33/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2543 - acc: 0.9978\n",
            "Epoch 34/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2541 - acc: 0.9978\n",
            "Epoch 35/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2540 - acc: 0.9977\n",
            "Epoch 36/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2538 - acc: 0.9978\n",
            "Epoch 37/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2536 - acc: 0.9981\n",
            "Epoch 38/50\n",
            "7263/7263 [==============================] - 0s 19us/step - loss: 0.2534 - acc: 0.9979\n",
            "Epoch 39/50\n",
            "7263/7263 [==============================] - 0s 22us/step - loss: 0.2533 - acc: 0.9982\n",
            "Epoch 40/50\n",
            "7263/7263 [==============================] - 0s 24us/step - loss: 0.2531 - acc: 0.9982\n",
            "Epoch 41/50\n",
            "7263/7263 [==============================] - 0s 22us/step - loss: 0.2529 - acc: 0.9988\n",
            "Epoch 42/50\n",
            "7263/7263 [==============================] - 0s 20us/step - loss: 0.2529 - acc: 0.9986\n",
            "Epoch 43/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2528 - acc: 0.9988\n",
            "Epoch 44/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2526 - acc: 0.9988\n",
            "Epoch 45/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2528 - acc: 0.9983\n",
            "Epoch 46/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2524 - acc: 0.9992\n",
            "Epoch 47/50\n",
            "7263/7263 [==============================] - 0s 22us/step - loss: 0.2523 - acc: 0.9990\n",
            "Epoch 48/50\n",
            "7263/7263 [==============================] - 0s 22us/step - loss: 0.2523 - acc: 0.9986\n",
            "Epoch 49/50\n",
            "7263/7263 [==============================] - 0s 21us/step - loss: 0.2522 - acc: 0.9988\n",
            "Epoch 50/50\n",
            "7263/7263 [==============================] - 0s 23us/step - loss: 0.2520 - acc: 0.9992\n",
            "acc: 0.98%\n",
            "4\n",
            "Epoch 1/50\n",
            "7248/7248 [==============================] - 1s 95us/step - loss: 0.5008 - acc: 0.6111\n",
            "Epoch 2/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.4207 - acc: 0.7889\n",
            "Epoch 3/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.3700 - acc: 0.8693\n",
            "Epoch 4/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.3362 - acc: 0.9185\n",
            "Epoch 5/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.3147 - acc: 0.9459\n",
            "Epoch 6/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.3010 - acc: 0.9581\n",
            "Epoch 7/50\n",
            "7248/7248 [==============================] - 0s 22us/step - loss: 0.2920 - acc: 0.9639\n",
            "Epoch 8/50\n",
            "7248/7248 [==============================] - 0s 20us/step - loss: 0.2855 - acc: 0.9699\n",
            "Epoch 9/50\n",
            "7248/7248 [==============================] - 0s 20us/step - loss: 0.2809 - acc: 0.9723\n",
            "Epoch 10/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2770 - acc: 0.9771\n",
            "Epoch 11/50\n",
            "7248/7248 [==============================] - 0s 20us/step - loss: 0.2739 - acc: 0.9801\n",
            "Epoch 12/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2714 - acc: 0.9839\n",
            "Epoch 13/50\n",
            "7248/7248 [==============================] - 0s 23us/step - loss: 0.2693 - acc: 0.9854\n",
            "Epoch 14/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2675 - acc: 0.9872\n",
            "Epoch 15/50\n",
            "7248/7248 [==============================] - 0s 22us/step - loss: 0.2658 - acc: 0.9901\n",
            "Epoch 16/50\n",
            "7248/7248 [==============================] - 0s 24us/step - loss: 0.2644 - acc: 0.9910\n",
            "Epoch 17/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2632 - acc: 0.9909\n",
            "Epoch 18/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2621 - acc: 0.9927\n",
            "Epoch 19/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2611 - acc: 0.9934\n",
            "Epoch 20/50\n",
            "7248/7248 [==============================] - 0s 23us/step - loss: 0.2601 - acc: 0.9945\n",
            "Epoch 21/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2594 - acc: 0.9943\n",
            "Epoch 22/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2588 - acc: 0.9949\n",
            "Epoch 23/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2582 - acc: 0.9954\n",
            "Epoch 24/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2577 - acc: 0.9959\n",
            "Epoch 25/50\n",
            "7248/7248 [==============================] - 0s 22us/step - loss: 0.2573 - acc: 0.9954\n",
            "Epoch 26/50\n",
            "7248/7248 [==============================] - 0s 23us/step - loss: 0.2568 - acc: 0.9960\n",
            "Epoch 27/50\n",
            "7248/7248 [==============================] - 0s 24us/step - loss: 0.2563 - acc: 0.9963\n",
            "Epoch 28/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2558 - acc: 0.9966\n",
            "Epoch 29/50\n",
            "7248/7248 [==============================] - 0s 23us/step - loss: 0.2556 - acc: 0.9972\n",
            "Epoch 30/50\n",
            "7248/7248 [==============================] - 0s 23us/step - loss: 0.2552 - acc: 0.9974\n",
            "Epoch 31/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2549 - acc: 0.9974\n",
            "Epoch 32/50\n",
            "7248/7248 [==============================] - 0s 22us/step - loss: 0.2547 - acc: 0.9977\n",
            "Epoch 33/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2544 - acc: 0.9981\n",
            "Epoch 34/50\n",
            "7248/7248 [==============================] - 0s 23us/step - loss: 0.2542 - acc: 0.9978\n",
            "Epoch 35/50\n",
            "7248/7248 [==============================] - 0s 20us/step - loss: 0.2540 - acc: 0.9982\n",
            "Epoch 36/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2539 - acc: 0.9978\n",
            "Epoch 37/50\n",
            "7248/7248 [==============================] - 0s 22us/step - loss: 0.2537 - acc: 0.9979\n",
            "Epoch 38/50\n",
            "7248/7248 [==============================] - 0s 22us/step - loss: 0.2534 - acc: 0.9981\n",
            "Epoch 39/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2533 - acc: 0.9983\n",
            "Epoch 40/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2531 - acc: 0.9983\n",
            "Epoch 41/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2529 - acc: 0.9985\n",
            "Epoch 42/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2528 - acc: 0.9988\n",
            "Epoch 43/50\n",
            "7248/7248 [==============================] - 0s 23us/step - loss: 0.2527 - acc: 0.9986\n",
            "Epoch 44/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2526 - acc: 0.9985\n",
            "Epoch 45/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2525 - acc: 0.9988\n",
            "Epoch 46/50\n",
            "7248/7248 [==============================] - 0s 20us/step - loss: 0.2524 - acc: 0.9989\n",
            "Epoch 47/50\n",
            "7248/7248 [==============================] - 0s 20us/step - loss: 0.2524 - acc: 0.9988\n",
            "Epoch 48/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2522 - acc: 0.9990\n",
            "Epoch 49/50\n",
            "7248/7248 [==============================] - 0s 20us/step - loss: 0.2521 - acc: 0.9992\n",
            "Epoch 50/50\n",
            "7248/7248 [==============================] - 0s 21us/step - loss: 0.2520 - acc: 0.9993\n",
            "acc: 0.99%\n",
            "4\n",
            "Epoch 1/50\n",
            "7261/7261 [==============================] - 1s 97us/step - loss: 0.5131 - acc: 0.5707\n",
            "Epoch 2/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.4296 - acc: 0.7677\n",
            "Epoch 3/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.3847 - acc: 0.8478\n",
            "Epoch 4/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.3519 - acc: 0.8993\n",
            "Epoch 5/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.3285 - acc: 0.9327\n",
            "Epoch 6/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.3125 - acc: 0.9482\n",
            "Epoch 7/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.3012 - acc: 0.9584\n",
            "Epoch 8/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2933 - acc: 0.9652\n",
            "Epoch 9/50\n",
            "7261/7261 [==============================] - 0s 23us/step - loss: 0.2874 - acc: 0.9694\n",
            "Epoch 10/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2827 - acc: 0.9727\n",
            "Epoch 11/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2791 - acc: 0.9759\n",
            "Epoch 12/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2759 - acc: 0.9787\n",
            "Epoch 13/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2734 - acc: 0.9817\n",
            "Epoch 14/50\n",
            "7261/7261 [==============================] - 0s 24us/step - loss: 0.2712 - acc: 0.9840\n",
            "Epoch 15/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2693 - acc: 0.9858\n",
            "Epoch 16/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2678 - acc: 0.9865\n",
            "Epoch 17/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2662 - acc: 0.9890\n",
            "Epoch 18/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2649 - acc: 0.9901\n",
            "Epoch 19/50\n",
            "7261/7261 [==============================] - 0s 23us/step - loss: 0.2638 - acc: 0.9905\n",
            "Epoch 20/50\n",
            "7261/7261 [==============================] - 0s 23us/step - loss: 0.2628 - acc: 0.9920\n",
            "Epoch 21/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2618 - acc: 0.9924\n",
            "Epoch 22/50\n",
            "7261/7261 [==============================] - 0s 24us/step - loss: 0.2609 - acc: 0.9934\n",
            "Epoch 23/50\n",
            "7261/7261 [==============================] - 0s 22us/step - loss: 0.2602 - acc: 0.9938\n",
            "Epoch 24/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2594 - acc: 0.9941\n",
            "Epoch 25/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2588 - acc: 0.9948\n",
            "Epoch 26/50\n",
            "7261/7261 [==============================] - 0s 25us/step - loss: 0.2583 - acc: 0.9956\n",
            "Epoch 27/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2579 - acc: 0.9959\n",
            "Epoch 28/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2574 - acc: 0.9963\n",
            "Epoch 29/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2567 - acc: 0.9971\n",
            "Epoch 30/50\n",
            "7261/7261 [==============================] - 0s 22us/step - loss: 0.2565 - acc: 0.9971\n",
            "Epoch 31/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2561 - acc: 0.9978\n",
            "Epoch 32/50\n",
            "7261/7261 [==============================] - 0s 23us/step - loss: 0.2557 - acc: 0.9974\n",
            "Epoch 33/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2554 - acc: 0.9970\n",
            "Epoch 34/50\n",
            "7261/7261 [==============================] - 0s 22us/step - loss: 0.2551 - acc: 0.9977\n",
            "Epoch 35/50\n",
            "7261/7261 [==============================] - 0s 23us/step - loss: 0.2550 - acc: 0.9974\n",
            "Epoch 36/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2546 - acc: 0.9978\n",
            "Epoch 37/50\n",
            "7261/7261 [==============================] - 0s 24us/step - loss: 0.2544 - acc: 0.9978\n",
            "Epoch 38/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2542 - acc: 0.9979\n",
            "Epoch 39/50\n",
            "7261/7261 [==============================] - 0s 23us/step - loss: 0.2540 - acc: 0.9977\n",
            "Epoch 40/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2539 - acc: 0.9979\n",
            "Epoch 41/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2536 - acc: 0.9982\n",
            "Epoch 42/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2535 - acc: 0.9981\n",
            "Epoch 43/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2533 - acc: 0.9985\n",
            "Epoch 44/50\n",
            "7261/7261 [==============================] - 0s 22us/step - loss: 0.2532 - acc: 0.9982\n",
            "Epoch 45/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2530 - acc: 0.9983\n",
            "Epoch 46/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2529 - acc: 0.9983\n",
            "Epoch 47/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2528 - acc: 0.9983\n",
            "Epoch 48/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2526 - acc: 0.9988\n",
            "Epoch 49/50\n",
            "7261/7261 [==============================] - 0s 21us/step - loss: 0.2525 - acc: 0.9988\n",
            "Epoch 50/50\n",
            "7261/7261 [==============================] - 0s 20us/step - loss: 0.2524 - acc: 0.9986\n",
            "acc: 0.99%\n",
            "4\n",
            "Epoch 1/50\n",
            "7262/7262 [==============================] - 1s 105us/step - loss: 0.5130 - acc: 0.5790\n",
            "Epoch 2/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.4301 - acc: 0.7680\n",
            "Epoch 3/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.3780 - acc: 0.8554\n",
            "Epoch 4/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.3418 - acc: 0.9117\n",
            "Epoch 5/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.3186 - acc: 0.9394\n",
            "Epoch 6/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.3036 - acc: 0.9543\n",
            "Epoch 7/50\n",
            "7262/7262 [==============================] - 0s 22us/step - loss: 0.2939 - acc: 0.9638\n",
            "Epoch 8/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2870 - acc: 0.9690\n",
            "Epoch 9/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2818 - acc: 0.9733\n",
            "Epoch 10/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2777 - acc: 0.9770\n",
            "Epoch 11/50\n",
            "7262/7262 [==============================] - 0s 24us/step - loss: 0.2745 - acc: 0.9793\n",
            "Epoch 12/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2718 - acc: 0.9828\n",
            "Epoch 13/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2694 - acc: 0.9861\n",
            "Epoch 14/50\n",
            "7262/7262 [==============================] - 0s 22us/step - loss: 0.2674 - acc: 0.9884\n",
            "Epoch 15/50\n",
            "7262/7262 [==============================] - 0s 22us/step - loss: 0.2657 - acc: 0.9897\n",
            "Epoch 16/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2641 - acc: 0.9912\n",
            "Epoch 17/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2628 - acc: 0.9924\n",
            "Epoch 18/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2617 - acc: 0.9924\n",
            "Epoch 19/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2607 - acc: 0.9941\n",
            "Epoch 20/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2597 - acc: 0.9945\n",
            "Epoch 21/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2590 - acc: 0.9945\n",
            "Epoch 22/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2582 - acc: 0.9960\n",
            "Epoch 23/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2575 - acc: 0.9966\n",
            "Epoch 24/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2570 - acc: 0.9968\n",
            "Epoch 25/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2565 - acc: 0.9972\n",
            "Epoch 26/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2561 - acc: 0.9968\n",
            "Epoch 27/50\n",
            "7262/7262 [==============================] - 0s 22us/step - loss: 0.2556 - acc: 0.9971\n",
            "Epoch 28/50\n",
            "7262/7262 [==============================] - 0s 22us/step - loss: 0.2552 - acc: 0.9977\n",
            "Epoch 29/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2549 - acc: 0.9979\n",
            "Epoch 30/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2545 - acc: 0.9983\n",
            "Epoch 31/50\n",
            "7262/7262 [==============================] - 0s 22us/step - loss: 0.2544 - acc: 0.9977\n",
            "Epoch 32/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2540 - acc: 0.9982\n",
            "Epoch 33/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2538 - acc: 0.9981\n",
            "Epoch 34/50\n",
            "7262/7262 [==============================] - 0s 22us/step - loss: 0.2536 - acc: 0.9985\n",
            "Epoch 35/50\n",
            "7262/7262 [==============================] - 0s 24us/step - loss: 0.2534 - acc: 0.9985\n",
            "Epoch 36/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2532 - acc: 0.9988\n",
            "Epoch 37/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2531 - acc: 0.9983\n",
            "Epoch 38/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2528 - acc: 0.9992\n",
            "Epoch 39/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2526 - acc: 0.9989\n",
            "Epoch 40/50\n",
            "7262/7262 [==============================] - 0s 23us/step - loss: 0.2525 - acc: 0.9989\n",
            "Epoch 41/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2523 - acc: 0.9990\n",
            "Epoch 42/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2522 - acc: 0.9989\n",
            "Epoch 43/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2521 - acc: 0.9993\n",
            "Epoch 44/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2520 - acc: 0.9989\n",
            "Epoch 45/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2519 - acc: 0.9992\n",
            "Epoch 46/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2518 - acc: 0.9992\n",
            "Epoch 47/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2517 - acc: 0.9992\n",
            "Epoch 48/50\n",
            "7262/7262 [==============================] - 0s 20us/step - loss: 0.2517 - acc: 0.9992\n",
            "Epoch 49/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2516 - acc: 0.9993\n",
            "Epoch 50/50\n",
            "7262/7262 [==============================] - 0s 21us/step - loss: 0.2515 - acc: 0.9993\n",
            "acc: 0.99%\n",
            "0.99% (+/- 0.00%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8b7134a2fe31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f%% (+/- %.2f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mvector_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mvector_plot_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vector_plot' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iDZ1kNQMNOZ",
        "colab_type": "text"
      },
      "source": [
        "#Tipos de treinamento e suas respectivas acurácias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkl1uQ_73kMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "plt.barh(vector_training,vector_plot,color='red',xerr = vector_plot_error, edgecolor='black',)\n",
        "plt.ylabel('Processos de Treinamento')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Processos vs Acurácia')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFNDU6Ix99D",
        "colab_type": "text"
      },
      "source": [
        "## Classificador de Sedentarismo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OJB2l7aJQSy",
        "colab_type": "text"
      },
      "source": [
        "### Contador de Constância "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PE4TpmrAgfS",
        "colab_type": "text"
      },
      "source": [
        "Verifica quantas vezes a pessoa fez alguma atividade em um período contínuo de, no mínimo, 10 minutos.\n",
        "\n",
        "Recebe 7 arquivos do **Classificador de Ações**, em que cada um representa uma lista  de ações detectadas em *1 dia* e em que cada ação representa *10 segundos* daquela atividade.\n",
        "\n",
        "Por exemplo, uma lista como:\n",
        "\n",
        "`['INATIVO','INATIVO','MODERADA','VIGOROSA','MODERADA',..., 'INATIVO']`\n",
        "\n",
        "Retorna uma lista de sub-listas, em que cada sublista representa um dia. Cada sub-lista é uma lista de tuplas, em que cada tupla representa um período de atividade contínua de alguma ação. O primeiro elemento da tupla é a string da ação e o segundo elemento é o tempo, em minutos, que aquela ação foi realizada de forma contínua (com uma certa tolerância). \n",
        "\n",
        "Por exemplo, *retorna* :\n",
        "\n",
        "`[list_dia1,list_dia2, ... ,list_dia7]`\n",
        "\n",
        "em que cada lista está no seguinte modelo,\n",
        "\n",
        "`list_dia1 = [('INATIVO',37.2),('MODERADA',15),('INATIVO',120.2)]`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptaS36aMJg1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "#import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbE0RJ26JinM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = 'lista_de_acoes.csv'\n",
        "delimiter = ';'\n",
        "big_window = 60 #10 minutes\n",
        "time_element = 10 #seconds\n",
        "classes = ['CAMINHADA','INATIVO','MODERADA','VIGOROSA']\n",
        "asserts = [0.94,0.95,0.9,0.96] #Accuracy of each class in the Stock Classifier model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkvdHMyUJj-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def csv_from_list(file_name):\n",
        "  with open(file_name) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file,delimiter=delimiter)\n",
        "    l = list(csv_reader)\n",
        "    return l\n",
        "\n",
        "\n",
        "def occurrences(period, classes):\n",
        "  n_class = len(classes)\n",
        "  n_period = len(period)\n",
        "  occ = [0] * n_class\n",
        "\n",
        "  for i in range(n_period):\n",
        "    try:\n",
        "      ind = classes.index(period[i])\n",
        "      occ[ind] += 1\n",
        "    except ValueError:\n",
        "      pass\n",
        "  return occ\n",
        "\n",
        "\n",
        "def occurrences_update(occ_old, period_old, period_new, classes,n_terms_new=1):\n",
        "  occ_new = occ_old[:]\n",
        "  N = len(period_new)\n",
        "  for i in range(n_terms_new):\n",
        "    try:\n",
        "      ind = classes.index(period_old[i])\n",
        "      occ_new[ind] -= 1\n",
        "    except ValueError:\n",
        "      pass\n",
        "  \n",
        "  for i in range(N-n_terms_new,N):\n",
        "    try:\n",
        "      ind = classes.index(period_new[i])\n",
        "      occ_new[ind] += 1\n",
        "    except ValueError:\n",
        "      pass\n",
        "\n",
        "  return occ_new\n",
        "\n",
        "\n",
        "def class_prediminantly(occ,classes,ret_ind_rel=False):\n",
        "  cddt = max(occ) #relevance candidate\n",
        "  \n",
        "  ind_rel = occ.index(cddt)\n",
        "\n",
        "  if ret_ind_rel:\n",
        "    return classes[ind_rel], ind_rel\n",
        "\n",
        "  else:\n",
        "    return classes[ind_rel]\n",
        "\n",
        "\n",
        "def relevant(occurrences,classes,asserts):\n",
        "  occ = occurrences\n",
        "  n = sum(occ)\n",
        "  cddt = max(occ) #relevance candidate\n",
        "  \n",
        "  class_rel,ind_rel = class_prediminantly(occ,classes,ret_ind_rel=True)\n",
        "  \n",
        "  rel_min = int(asserts[ind_rel] * n)\n",
        "\n",
        "  #rel = False\n",
        "\n",
        "  if cddt >= rel_min:\n",
        "    return class_rel\n",
        "\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def interval_div(l):\n",
        "  ls = []\n",
        "  n = len(l)\n",
        "  i = 1\n",
        "  l_aux = [l[0]]\n",
        "  while i < n:\n",
        "    if l[i] == l[i-1]+1:\n",
        "      l_aux.append(l[i])\n",
        "    else:\n",
        "      ls.append(l_aux)\n",
        "      l_aux = [l[i]]\n",
        "    \n",
        "    i+=1\n",
        "  ls.append(l_aux)\n",
        "  return ls\n",
        "\n",
        "\n",
        "def list_index_for_blocks(l,l_inds,big_window,time_element):\n",
        "  n_blk = len(l_inds)\n",
        "  blks = []\n",
        "  for i_blk in range(n_blk):\n",
        "    inds = l_inds[i_blk]\n",
        "    i0 = inds[0]\n",
        "    blk = l[i0:i0+big_window]\n",
        "\n",
        "    for i in inds[1:]:\n",
        "      blk.append(l[i+big_window])\n",
        "    \n",
        "    blks.append(blk)\n",
        "\n",
        "  return blks\n",
        "\n",
        "\n",
        "def blocks_and_times(rough_blks,time_element,classes,asserts):\n",
        "  n_blks = len(rough_blks)\n",
        "  blks = []\n",
        "  for i in range(n_blks):\n",
        "    r_blk = rough_blks[i]\n",
        "    n_blk = len(r_blk)\n",
        "    occ = occurrences(r_blk,classes)\n",
        "    clss = class_prediminantly(occ,classes)\n",
        "\n",
        "    t_blk = time_element*max(occ)\n",
        "\n",
        "    blks.append((clss,t_blk/60))\n",
        "\n",
        "  return blks\n",
        "\n",
        "def one_day_cont(day_list, classes, big_window, asserts,time_element):\n",
        "  \"Analyzes a one-day dataset.\"\n",
        "  N = len(day_list)\n",
        "  n_class = len(classes)\n",
        "  period_0 = day_list[:big_window]\n",
        "  n_terms_new = 1\n",
        "\n",
        "  inds_rel = []\n",
        "  rels = []\n",
        "\n",
        "  for i in range(N-big_window):\n",
        "    period = day_list[i:i+big_window]\n",
        "\n",
        "    if i == 0:\n",
        "      occ = occurrences(period_0,classes)\n",
        "    else:\n",
        "      occ = occurrences_update(occ_old,period_old,period,classes,n_terms_new)\n",
        "    \n",
        "    rel = relevant(occ,classes,asserts)\n",
        "    if rel:\n",
        "      inds_rel.append(i)\n",
        "      rels.append(rel)\n",
        "\n",
        "    occ_old = occ\n",
        "    period_old = period    \n",
        "\n",
        "  inds_blocks = interval_div(inds_rel)\n",
        "  rough_blks = list_index_for_blocks(day_list,inds_blocks,big_window,time_element)\n",
        "\n",
        "  blocks = blocks_and_times(rough_blks,time_element,classes,asserts)\n",
        "\n",
        "  return blocks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXM4RAShabpg",
        "colab_type": "text"
      },
      "source": [
        "## Testes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCFH8dtMuXRh",
        "colab_type": "text"
      },
      "source": [
        "Importando arquivo teste gerado aleatoriamente com apenas 60 strings respectivamente iguais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRqBVf5YtSEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6ba39d3a-f8dd-4a87-ddb3-66e97bc83480"
      },
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/pessoa-poli/MJVictory/master/lista_de_acoes.csv\"\n",
        "df1 = pd.read_csv(url, header=None)\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>960</th>\n",
              "      <th>961</th>\n",
              "      <th>962</th>\n",
              "      <th>963</th>\n",
              "      <th>964</th>\n",
              "      <th>965</th>\n",
              "      <th>966</th>\n",
              "      <th>967</th>\n",
              "      <th>968</th>\n",
              "      <th>969</th>\n",
              "      <th>970</th>\n",
              "      <th>971</th>\n",
              "      <th>972</th>\n",
              "      <th>973</th>\n",
              "      <th>974</th>\n",
              "      <th>975</th>\n",
              "      <th>976</th>\n",
              "      <th>977</th>\n",
              "      <th>978</th>\n",
              "      <th>979</th>\n",
              "      <th>980</th>\n",
              "      <th>981</th>\n",
              "      <th>982</th>\n",
              "      <th>983</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>...</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1        2        3    ...       996       997       998       999\n",
              "0  INATIVO  INATIVO  INATIVO  INATIVO  ...  VIGOROSA  VIGOROSA  VIGOROSA  VIGOROSA\n",
              "\n",
              "[1 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcCo9kqMtW-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "day_test = list(df1.iloc[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOjvHLgLtYxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = one_day_cont(day_test,classes,big_window,asserts,time_element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmsrinFDvt1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dd553594-fd54-4404-9fd8-85105c969bda"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('INATIVO', 106.0),\n",
              " ('VIGOROSA', 14.833333333333334),\n",
              " ('INATIVO', 30.333333333333332),\n",
              " ('VIGOROSA', 14.833333333333334)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tco7kOzHgfYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e5d6c4f3-6ee0-4ec7-edfa-387a29a7ebfe"
      },
      "source": [
        "url2 = \"https://raw.githubusercontent.com/pessoa-poli/MJVictory/master/lista_de_acoes.csv\"\n",
        "df2 = pd.read_csv(url2, header=None)\n",
        "print(df2.count(axis=1))\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1000\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>960</th>\n",
              "      <th>961</th>\n",
              "      <th>962</th>\n",
              "      <th>963</th>\n",
              "      <th>964</th>\n",
              "      <th>965</th>\n",
              "      <th>966</th>\n",
              "      <th>967</th>\n",
              "      <th>968</th>\n",
              "      <th>969</th>\n",
              "      <th>970</th>\n",
              "      <th>971</th>\n",
              "      <th>972</th>\n",
              "      <th>973</th>\n",
              "      <th>974</th>\n",
              "      <th>975</th>\n",
              "      <th>976</th>\n",
              "      <th>977</th>\n",
              "      <th>978</th>\n",
              "      <th>979</th>\n",
              "      <th>980</th>\n",
              "      <th>981</th>\n",
              "      <th>982</th>\n",
              "      <th>983</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>INATIVO</td>\n",
              "      <td>...</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "      <td>VIGOROSA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1        2        3    ...       996       997       998       999\n",
              "0  INATIVO  INATIVO  INATIVO  INATIVO  ...  VIGOROSA  VIGOROSA  VIGOROSA  VIGOROSA\n",
              "\n",
              "[1 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uRmYO3rgojU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "day_test2 = list(df2.iloc[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYqEVZhzllyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42111539-0750-4c91-a544-49d993a77059"
      },
      "source": [
        "i= 0\n",
        "for e in day_test2:\n",
        "  if e == 'INATIVO':\n",
        "    i+=1\n",
        "i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "820"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FBbGSLTgvIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result2 = one_day_cont(day_test2,classes,big_window,asserts,time_element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJBpI0ocg5Da",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dc85010e-d190-4942-d4b4-b038e3d616f5"
      },
      "source": [
        "result2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('INATIVO', 106.0),\n",
              " ('VIGOROSA', 14.833333333333334),\n",
              " ('INATIVO', 30.333333333333332),\n",
              " ('VIGOROSA', 14.833333333333334)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u26rjhvnQ8rs",
        "colab_type": "text"
      },
      "source": [
        "#Classificação de pessoas em níveis de Sedentarismo com base no Tipo de Ação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufNWeyRfRR_v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Casos de Teste\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XckzaGRVcX5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Casos de Uso para cada classe de sedentarismo\n",
        "pessoa_sedentario = [[('INATIVO',11.5),('INATIVO',15.5)],[],[('INATIVO',21.5),('INATIVO',10.5)],[('INATIVO',21.5),('INATIVO',10.5)],[('INATIVO',21.5),('INATIVO',10.5)],[('INATIVO',21.5),('INATIVO',10.5)],[('INATIVO',21.5),('INATIVO',10.5)]]\n",
        "pessoa_muitoAtivo = [\n",
        "                     [('VIGOROSA',10.5),('CAMINHADA',30.5),('INATIVO',11.5),('INATIVO',11.5)], #cada linha representa um dia.\n",
        "                     [],\n",
        "                     [('VIGOROSA',30.5),('CAMINHADA',30.5),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('VIGOROSA',30.5),('CAMINHADA',30.5),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('VIGOROSA',30.5),('CAMINHADA',30.5),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('CAMINHADA',30.5),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',30.5),('INATIVO',21.5),('INATIVO',10.5)]\n",
        "                     ]\n",
        "\n",
        "pessoa_Ativo = [\n",
        "                     [('VIGOROSA',20.5),('CAMINHADA',130),('INATIVO',11.5),('INATIVO',11.5)],\n",
        "                     [],\n",
        "                     [('INATIVO',20.5),('VIGOROSA',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',20.5),('VIGOROSA',10.5),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('VIGOROSA',10.5),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('VIGOROSA',10.5),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('VIGOROSA',10.5),('INATIVO',21.5),('INATIVO',10.5)]\n",
        "                     ]\n",
        "\n",
        "pessoa_Insuficiente_A_1 = [\n",
        "                     [('INATIVO',20.5),('MODERADA',10),('INATIVO',11.5),('INATIVO',11.5)],\n",
        "                     [],\n",
        "                     [('INATIVO',20.5),('MODERADA',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',20.5),('MODERADA',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('MODERADA',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('MODERADA',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)]\n",
        "                     ]\n",
        "pessoa_Insuficiente_A_2 = [\n",
        "                     [('INATIVO',20.5),('MODERADA',150),('INATIVO',11.5),('INATIVO',11.5)],\n",
        "                     [],\n",
        "                     [('INATIVO',20.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',20.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)]\n",
        "                     ]\n",
        "pessoa_Insuficiente_B = [\n",
        "                     [('INATIVO',20.5),('MODERADA',10),('INATIVO',11.5),('INATIVO',11.5)],\n",
        "                     [],\n",
        "                     [('INATIVO',20.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',20.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)],\n",
        "                     [('INATIVO',30.5),('INATIVO',10),('INATIVO',21.5),('INATIVO',10.5)]\n",
        "                     ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebK-L7W2RsqW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Funções\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPo0LzFYSHfa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        ">> Classe Sedentário\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD5jRQfaqu30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função: sedentario:\n",
        "#\n",
        "# Descrição: Verifica se a pessoa é sedentária, ou seja,não realiza nenhuma atividade física por pelo menos 10 minutos contínuos durante a semana;\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se não pertence a classe sedentário, 1 caso contrário\n",
        "\n",
        "def sedentario(pessoa):\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "      if atividade[0]==\"VIGOROSA\" or atividade[0]==\"MODERADA\" or atividade[0]==\"CAMINHADA\":  \n",
        "        return 0\n",
        "\n",
        "  return 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udolnSkaSQhp",
        "colab_type": "text"
      },
      "source": [
        ">> Classe Muito Ativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi0OT_OUc3ZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função: muito_ativo_cond_a:\n",
        "#\n",
        "# Descrição: Verifica se a pessoa cumpre a restrição (a) da classe muito_ativo, ou seja, faz atividades vigorosas – ≥ 5 dias/semana e ≥ 30 min/sessão\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se cumpre restrição, 1 caso contrário.\n",
        "\n",
        "def muito_ativo_cond_a(pessoa):\n",
        "  #condição (a)\n",
        "  trinta_min_atividade=0\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "     \n",
        "      if atividade[0]==\"VIGOROSA\" and atividade[1]>=30:  \n",
        "          trinta_min_atividade=trinta_min_atividade+1\n",
        "          break\n",
        "  if trinta_min_atividade >= 5:     \n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "# Função: muito_ativo_cond_b:\n",
        "#\n",
        "# Descrição: Verifica se a pessoa cumpre a restrição (b) da classe muito_ativo,\n",
        "#             ou seja, vigorosa – ≥ 3 dias/ semana e ≥ 20 min/sessão + moderada\n",
        "#             e ou caminhada ≥ 5 dias/semana e ≥ 30 min/sessão.\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se cumpre restrição, 1 caso contrário.\n",
        "\n",
        "def muito_ativo_cond_b(pessoa):\n",
        "  #condicao(b)\n",
        "  vinte_min_atividade_VIGOROSA=0\n",
        "  trinta_min_atividade_MODERADA=0\n",
        "  trinta_min_atividade_CAMINHADA=0\n",
        "  lista_dias=[]\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "      if atividade[0]==\"VIGOROSA\" and atividade[1]>=20:  \n",
        "         vinte_min_atividade_VIGOROSA= vinte_min_atividade_VIGOROSA+1\n",
        "         break\n",
        "\n",
        "  for dia in pessoa:\n",
        "    for atividade1 in dia:\n",
        "      if atividade1[0]==\"MODERADA\" and atividade1[1]>=30:  \n",
        "        trinta_min_atividade_MODERADA=trinta_min_atividade_MODERADA+1\n",
        "        lista_dias.append(dia)\n",
        "        break\n",
        "\n",
        "  for dia in pessoa:\n",
        "    if dia in lista_dias:\n",
        "      continue\n",
        "    for atividade1 in dia:\n",
        "      if atividade1[0]==\"CAMINHADA\" and atividade1[1]>=30:  \n",
        "        trinta_min_atividade_CAMINHADA=trinta_min_atividade_CAMINHADA+1\n",
        "        break\n",
        "\n",
        "  soma_caminhada_moderada = trinta_min_atividade_MODERADA + trinta_min_atividade_CAMINHADA\n",
        "\n",
        "  if soma_caminhada_moderada >= 5 and vinte_min_atividade_VIGOROSA >=3: \n",
        "    #print(trinta_min_atividade_MODERADA,vinte_min_atividade_VIGOROSA)\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "# Função: muitoAtivo:\n",
        "#\n",
        "# Descrição: Verifica se a pessoa cumpre a restrição (a) ou (b) da class muito_ativo.\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se não cumpre (a) e (b), 1 caso contrário.\n",
        "\n",
        "def muitoAtivo(pessoa):\n",
        "    b = muito_ativo_cond_b(pessoa)\n",
        "    a = muito_ativo_cond_a(pessoa)\n",
        "    #print(a,b)\n",
        "    if a==1 or b==1 :\n",
        "      return 1\n",
        "    return 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vihsHMskSWmw",
        "colab_type": "text"
      },
      "source": [
        ">> Classe Ativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fULKQE9bftnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função: ativo_cond_a\n",
        "#\n",
        "# Descrição: Verifica se a pessoa cumpre a restrição (a) da class ativo,ou seja , realiza\n",
        "#            atividade física vigorosa – ≥ 3 dias/semana e ≥ 20 minutos/sessão; \n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se não cumpre , 1 caso contrário.\n",
        "def ativo_cond_a(pessoa):\n",
        "  #condição (a)\n",
        "  trinta_min_atividade=0\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "     \n",
        "      if atividade[0]==\"VIGOROSA\" and atividade[1]>=20:  \n",
        "          trinta_min_atividade=trinta_min_atividade+1\n",
        "          break\n",
        "  if trinta_min_atividade >= 3:     \n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def ativo_cond_b(pessoa):\n",
        "  #condição (b)\n",
        "  trinta_min_atividade=0\n",
        "  trinta_min_CAMINHADA=0\n",
        "  lista_dias=[]\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "     \n",
        "      if atividade[0]==\"MODERADA\" and atividade[1]>=30:  \n",
        "          trinta_min_atividade=trinta_min_atividade+1\n",
        "          break\n",
        "\n",
        "  for dia in pessoa:\n",
        "    if dia in lista_dias:\n",
        "      continue\n",
        "    for atividade in dia:\n",
        "     \n",
        "      if atividade[0]==\"CAMINHADA\" and atividade[1]>=30:  \n",
        "          trinta_min_CAMINHADA=trinta_min_CAMINHADA+1\n",
        "          break\n",
        "  if trinta_min_atividade + trinta_min_CAMINHADA  >= 5:     \n",
        "    return 1 \n",
        "  return 0\n",
        "\n",
        "# Função: ativo_cond_c\n",
        "#\n",
        "# Descrição: Verifica se a pessoa cumpre a restrição (c) da class ativo,ou seja , realiza\n",
        "#            Qualquer atividade cpm frequência ≥ 5 dias/sem e ≥150  minutos de exercicio/semana  (caminhada  +  moderada +vigorosa).\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se não cumpre , 1 caso contrário.\n",
        "def ativo_cond_c(pessoa):\n",
        "  #condição (c)\n",
        "  trinta_min_atividade=0\n",
        "  soma=0\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:    \n",
        "      if atividade[0]==\"MODERADA\" or atividade[0]==\"VIGOROSA\" or atividade[0]==\"CAMINHADA\" :  \n",
        "          trinta_min_atividade=trinta_min_atividade+1\n",
        "          break\n",
        "\n",
        "\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "      if atividade[0]==\"MODERADA\" or atividade[0]==\"VIGOROSA\" or atividade[0]==\"CAMINHADA\" :  \n",
        "          soma=soma+atividade[1]\n",
        "\n",
        "\n",
        "  if trinta_min_atividade >= 5 and soma >= 150:     \n",
        "    return 1 \n",
        "  return 0\n",
        "\n",
        "  \n",
        "# Função: Ativo:\n",
        "#\n",
        "# Descrição: Verifica se a pessoa cumpre a restrição (a) ou (b) ou (c) da class ativo.\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se não cumpre (a) e (b) e(c) 1 caso contrário.\n",
        "def Ativo(pessoa):\n",
        "  a =ativo_cond_a(pessoa)\n",
        "  b=ativo_cond_b(pessoa)\n",
        "  c=ativo_cond_c(pessoa)\n",
        "  if a or b or c:\n",
        "    return 1\n",
        "  return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37ecO9r4Sl6u",
        "colab_type": "text"
      },
      "source": [
        ">>Classes Insuficientemente Ativo A & B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiZ9OyXoh3gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função: insuficiente_ativo_a\n",
        "#\n",
        "# Descrição: Verifica se a pessoa Faz no mínimo 10  minutos de atividade física semanal continua e cumpre um dos requisitos abaixo\n",
        "#            (e não  é ativo nem muito ativo).\n",
        "#             a)Freqüência: 5 dias /semana ou\n",
        "#             b) Duração mínima de  150 min / semana\n",
        "\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se não cumpre (a) e (b), 1 caso contrário.\n",
        "def insuficiente_ativo_a(pessoa):\n",
        "  trinta_min_atividade=0\n",
        "  soma=0\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:    \n",
        "      if atividade[0]==\"MODERADA\" or atividade[0]==\"VIGOROSA\" or atividade[0]==\"CAMINHADA\" :  \n",
        "          trinta_min_atividade=trinta_min_atividade+1\n",
        "          break\n",
        "\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "      if atividade[0]==\"MODERADA\" or atividade[0]==\"VIGOROSA\" or atividade[0]==\"CAMINHADA\" :  \n",
        "          soma=soma+atividade[1]\n",
        "\n",
        "  if soma>=150 and  trinta_min_atividade >= 5:\n",
        "    return 0\n",
        "  if soma>=150 or  trinta_min_atividade >= 5:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "\n",
        "# Função: insuficiente_ativo_b\n",
        "#\n",
        "# Descrição: Verifica se a pessoa Faz no mínimo 10  minutos de atividade física semanal continua \n",
        "#            (e não  é inusuficientemente ativo A).\n",
        "\n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: 0 se não cumpre (a) e (b), 1 caso contrário.\n",
        "def insuficiente_ativo_b(pessoa):\n",
        "  flag=0\n",
        "  for dia in pessoa:\n",
        "    for atividade in dia:\n",
        "      if atividade[0]==\"VIGOROSA\" or atividade[0]==\"MODERADA\" or atividade[0]==\"CAMINHADA\" :  \n",
        "        flag=1\n",
        "\n",
        "  if not insuficiente_ativo_a(pessoa) and flag:\n",
        "    return 1\n",
        "  return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HhLbC2RTABE",
        "colab_type": "text"
      },
      "source": [
        ">>Função que retorna qual a classe & Função de teste dos casos de uso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P1j3hH_jV02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função: classifica\n",
        "#\n",
        "# Descrição: Determina a classe de sedentarismo da pessoa baseado nas suas atividades nos últimos 7 dias  \n",
        "#\n",
        "# Parametro: lista de listas contendo as atividades por dia\n",
        "#\n",
        "# Retorno: Nome da classe em formato de String \n",
        "def classifica(pessoa):\n",
        "  sedentario_ = sedentario(pessoa)\n",
        "  insuf_A = insuficiente_ativo_a(pessoa)\n",
        "  insuf_B = insuficiente_ativo_b(pessoa)\n",
        "  ativo = Ativo(pessoa)\n",
        "  muitoativo = muitoAtivo(pessoa)\n",
        "  if muitoativo==1: return \"Muito Ativo\"\n",
        "  if ativo==1: return \"Ativo\"\n",
        "  if insuf_A==1: return \"Insuficientemente Ativo A\"\n",
        "  if insuf_B==1: return \"Insuficientemente Ativo B\"\n",
        "  if sedentario_==1: return \"Sedentário\"\n",
        "  \n",
        "  \n",
        "  return \"Erro\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zesnatsjBWcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função: teste_function\n",
        "#\n",
        "# Descrição: Verifica se os casos de uso tem o resultado esperado.  \n",
        "#\n",
        "# Parametro: nenhum\n",
        "#\n",
        "# Retorno: String indicando quais classes foram corretamente identificadas e quais não foram\n",
        "\n",
        "def teste_function():\n",
        "  classificacao_correta=''\n",
        "  classificacao_errada=''\n",
        "  if classifica(pessoa_sedentario)==\"Sedentário\":\n",
        "    classificacao_correta=classificacao_correta+\"Sedentário ,\"\n",
        "  else:\n",
        "    classificacao_errada=classificacao_errada+\"Sedentário ,\"\n",
        "\n",
        "  if classifica(pessoa_muitoAtivo)==\"Muito Ativo\":\n",
        "    classificacao_correta=classificacao_correta+\"Muito Ativo, \"\n",
        "  else:\n",
        "    classificacao_errada=classificacao_errada+\"Muito Ativo, \"\n",
        "\n",
        "\n",
        "  if classifica(pessoa_Ativo)==\"Ativo\":\n",
        "    classificacao_correta=classificacao_correta+\"Ativo, \"\n",
        "  else:\n",
        "    classificacao_errada=classificacao_errada+\"Ativo, \"\n",
        "\n",
        "\n",
        "\n",
        "  if classifica(pessoa_Insuficiente_A_1 )==\"Insuficientemente Ativo A\":\n",
        "    classificacao_correta=classificacao_correta+\"Insuficientemente Ativo A, \"\n",
        "  else:\n",
        "    classificacao_errada=classificacao_errada+\"Insuficientemente Ativo A, \"\n",
        "\n",
        "\n",
        "  if classifica(pessoa_Insuficiente_B )==\"Insuficientemente Ativo B\":\n",
        "    classificacao_correta=classificacao_correta+\"Insuficientemente Ativo B, \"\n",
        "  else:\n",
        "    classificacao_errada=classificacao_errada+\"Insuficientemente Ativo B, \"\n",
        "\n",
        "\n",
        "  return \"Classificação correta: \"+classificacao_correta +\". Classificação Incorreta: \"+classificacao_errada"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U4xhBSvUHC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teste_function()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47r84hdtoge",
        "colab_type": "text"
      },
      "source": [
        "##Gerador de listas para teste piloto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QBozN_kvu03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import secrets\n",
        "import random\n",
        "\n",
        "classificacoes = ['VIGOROSA', 'MODERADA', 'INATIVO']\n",
        "#Lista com sequência repetida de vigorosa, moderada e inativa\n",
        "def cria_lista(size):\n",
        "  lista_output = []\n",
        "  t = 0\n",
        "\n",
        "  for i in range(size):\n",
        "    lista_output.append(classificacoes[t])\n",
        "    t+=1\n",
        "    if t == 3:\n",
        "      t = 0\n",
        "  return lista_output\n",
        "\n",
        "#Lista com todos itens iguais a uma classe escolhida\n",
        "def cria_lista2(size):\n",
        "  classe_escolhida = 'INATIVO'\n",
        "  lista_output = []  \n",
        "  for i in range(size):\n",
        "    lista_output.append(classe_escolhida)\n",
        "\n",
        "  return lista_output\n",
        "\n",
        "#Dada uma lista, mapeia espaços onde não existe continuidade de classes de tamanho mínimo gap_size.\n",
        "def map_empty(lista, gap_size):\n",
        "  selecao = lista[0]\n",
        "  count = 0\n",
        "  continuo = []\n",
        "  nao_continuo = []\n",
        "  start = 0\n",
        "  end = 1\n",
        "  for i in range(len(lista)):\n",
        "    if lista[i] == selecao and lista[i] != 'INATIVO':\n",
        "      count+=1\n",
        "    #Esse bloco identifica se peguei um gap de tamanho suficiente, e estende a identificação até o final do trecho contínuo.\n",
        "    if count == gap_size:\n",
        "      while lista[i] == selecao and i < (len(lista)-1):\n",
        "        end = i\n",
        "        i += 1\n",
        "      continuo.append((start,end))\n",
        "      count=0\n",
        "    if lista[i] != selecao:\n",
        "      start=i\n",
        "      selecao = lista[i]\n",
        "      count=1\n",
        "  if continuo == []:\n",
        "      nao_continuo = [(cat+gap_size*cat,cat+gap_size+gap_size*cat) for cat in range(int(len(lista)/gap_size))]\n",
        "  #Esse bloco cria o vetor de trechos não contínuos, com base no vetor de trechos contínuos.\n",
        "  \n",
        "  for t in range(len(continuo)):\n",
        "    if t == 0 and continuo[0][0] != 0:\n",
        "      nao_continuo.append((0,continuo[0][0]-1))\n",
        "    if t+1 != len(continuo):\n",
        "        nao_continuo.append((continuo[t][1]+1,continuo[t+1][0]-1))\n",
        "    if t == (len(continuo)-1) and continuo[t][1] != len(lista):\n",
        "      nao_continuo.append((continuo[t][1]+1,len(lista)))\n",
        "\n",
        "    \n",
        "  print(f'Não contínuo: {nao_continuo}')\n",
        "  print(f'Contínuo: {continuo}')\n",
        "  return nao_continuo\n",
        "\n",
        "#Função que insere gaps na lista gerada, com base nos espaçoes vazios mapeados por empty_gaps, dados através de uma lista de tuplas.\n",
        "def insere_gap(lista, qtd_gaps, gap_size, classificacao):\n",
        "  print(f'leninicio: {len(lista)}')\n",
        "  empty_gaps = map_empty(lista,gap_size)\n",
        "  gap_list = [classificacao for cat in range(gap_size)]\n",
        "  gaps_inseridos = 0\n",
        "  for i in range(qtd_gaps):\n",
        "    gap = secrets.choice(empty_gaps)    \n",
        "    if (gap[1]-gap[0]+1) >= gap_size:\n",
        "      lista = lista[0:gap[0]] + gap_list + lista[gap[1]:]\n",
        "      empty_gaps.remove(gap)\n",
        "      gaps_inseridos += 1\n",
        "  print(gaps_inseridos)\n",
        "  print(f'len_lista: {len(lista)}')\n",
        "  return lista\n",
        "\n",
        "#Definição de parâmetros para geração da lista\n",
        "minha_lista = cria_lista2(1000) #Cria lista com 1000 entradas  iguais a \"INATIVO\"\n",
        "tamanho_do_gap = 90 #Tamanho da sequência continua da classe escolhida\n",
        "numero_de_gaps = 2 #Número de sequências contínuas dentro da minha_lista\n",
        "classe = 'VIGOROSA' #Classe escolhida\n",
        "\n",
        "lista_out = insere_gap(minha_lista,numero_de_gaps,tamanho_do_gap,classe)\n",
        "\n",
        " #Crio um arquivo que conterá a lista criada.\n",
        "f = open('lista_de_acoes.csv', 'w')\n",
        "f.write(','.join(lista_out))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}